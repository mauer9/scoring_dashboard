{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing_new_features.py\n######################### TO-DO #########################\n# 1. User-ID based sessions?\n# 2. Redis database when in prod\n# 3. Add dark/light themes\n# 4. Add all models status page (JSON to store all model metrics on GitLab)\n\n#########################################################\n\nfrom dash import Dash, dcc, html, Input, Output, State\nfrom flask_caching import Cache\nimport dash_bootstrap_components as dbc\n# import dash_core_components as dcc\n# import dash_html_components as html\n# from dash.dependencies import Input, Output, State\nfrom dash import Dash, dcc, html, Input, Output, ctx\nimport dash_bootstrap_components as dbc\nimport plotly.express as px\n\nfrom plotting_functions import plot_hist_dist, plot_feature_importances, create_card, psi_plot_ly, plot_auc_roc, \\\n    plot_ks, psi_variable_plot, new_create_card\nfrom helper_functions import col_dropper, fix_dtypes, DataFrameImputer, AutoPrepareScoreCard\n# import plotly.graph_objects as go\nimport pandas as pd\n# from datetime import date\nimport sqlite3\nfrom sklearn.metrics import roc_curve, roc_auc_score, log_loss\nfrom sklearn.model_selection import train_test_split\nimport json\nimport helper_functions\nimport joblib\n\nfrom optbinning import BinningProcess\nfrom optbinning.scorecard import ScorecardMonitoring\nimport logging\nimport sys\nimport io\nfrom contextlib import redirect_stdout\nimport time\nfrom docxtpl import DocxTemplate, InlineImage\n\nimport credit_py_validation as cpv\n\nfrom plotly.tools import mpl_to_plotly\n\nfrom page_content import generate_search_bar, generate_navbar, generate_sidebar, generate_footer\nfrom styles import get_content_style, get_footer_style, get_sidebar_style, get_sidebar_hidden_style, get_modal_style\nimport pages.accuracy\nimport pages.stability\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DashLogger(logging.StreamHandler):\n    def __init__(self, stream=None):\n        super().__init__(stream=stream)\n        self.logs = list()\n\n    def emit(self, record):\n        try:\n            msg = self.format(record)\n            self.logs.append(msg)\n            self.logs = self.logs[-1000:]\n            self.flush()\n        except Exception:\n            self.handleError(record)\n\ndash_logger = DashLogger(stream=sys.stdout)\nlogger.addHandler(dash_logger)\n\n# Global variables\nHALYK_LOGO = 'assets/halyk_logo.png'\n# PRIVATE_TOKEN = ''\nmodel_list = helper_functions.get_all_models()\n\napp = Dash(external_stylesheets=[dbc.themes.CYBORG])\n\nnavbar = generate_navbar(model_list, HALYK_LOGO)\n\n# the style arguments for the sidebar. We use position:fixed and a fixed width\nCONTENT_STYLE, CONTENT_STYLE1 = get_content_style()\nFOOTER_STYLE = get_footer_style()\nSIDEBAR_STYLE = get_sidebar_style()\nSIDEBAR_HIDDEN = get_sidebar_hidden_style()\nMODAL_STYLE = get_modal_style()\n\nsidebar = generate_sidebar(SIDEBAR_STYLE)\n\ncontent = html.Div(\n    id=\"page-content\",\n    style=CONTENT_STYLE)\n\nfooter = generate_footer(FOOTER_STYLE)\n\ngenerate_button = html.Div(\n    html.Button('Generate report',\n                id='generate-report-button',\n                n_clicks=0))\n\nMODAL_CONTENT = {\n    \"margin\": \"90px\",\n    \"padding\": \"30px\",\n    \"background-color\": \"white\",\n    'textAlign': 'center'\n}\n\nmodal = html.Div([\n    html.Div([\n        html.Div([\n            'Please, choose a model from the dropdown on top, retard',\n        ]),\n\n        html.Hr(),\n        html.Button('Close', id='modal-close-button')\n    ],\n        style=MODAL_CONTENT,\n        className='modal-content',\n    ),\n],\n    id='modal',\n    className='modal',\n    style=MODAL_STYLE\n)\n\napp.layout = html.Div(\n    [\n        dcc.Store(id='intermediate-value'),\n        dcc.Store(id='side_click'),\n        dcc.Location(id=\"url\"),\n        navbar,\n        sidebar,\n        modal,\n\n        dcc.Loading(\n            id='loading-1',\n            type='default',\n            # fullscreen=True,\n            # children=html.Div(id='loading-output-1'),\n            # children=['intermediate-value', content]\n            children=content\n        ),\n        # content,\n\n        dcc.Interval(\n            id='log-update2',\n            interval=1 * 1000  # in milliseconds\n        ),\n        html.Div(id='log2'),\n\n        generate_button,\n        footer\n    ]\n)\n\n@app.callback(\n    Output('log2', 'children'),\n    [Input('log-update2', 'n_intervals')])\ndef update_logs(interval):\n    return [html.Div(log) for log in dash_logger.logs]\n\n\n# @app.callback(Output('my-div', 'children'), [Input('button', 'n_clicks')])\n# def add_log(click):\n#     logger.warning(\"Important Message\")\n\n\n\n\n\n@app.callback(Output('modal', 'style'),\n              [Input('modal-close-button', 'n_clicks')])\ndef close_modal(n):\n    if n is None:\n        return get_modal_style()\n    elif (n is not None) and (n > 0):\n        return {\"display\": \"none\"}\n\n\n@app.callback(\n    [\n        Output(\"sidebar\", \"style\"),\n        Output(\"page-content\", \"style\"),\n        Output(\"side_click\", \"data\"),\n    ],\n\n    [Input(\"btn_sidebar\", \"n_clicks\")],\n    [\n        State(\"side_click\", \"data\"),\n    ]\n)\ndef toggle_sidebar(n, nclick):\n    if n:\n        if nclick == \"SHOW\":\n            sidebar_style = SIDEBAR_HIDDEN\n            content_style = CONTENT_STYLE1\n            cur_nclick = \"HIDDEN\"\n        else:\n            sidebar_style = SIDEBAR_STYLE\n            content_style = CONTENT_STYLE\n            cur_nclick = \"SHOW\"\n    else:\n        sidebar_style = SIDEBAR_STYLE\n        content_style = CONTENT_STYLE\n        cur_nclick = 'SHOW'\n\n    return sidebar_style, content_style, cur_nclick\n\n\n# this callback uses the current pathname to set the active state of the\n# corresponding nav link to true, allowing users to tell see page they are on\n@app.callback(\n    [Output(f\"page-{i}-link\", \"active\") for i in range(1, 4)],\n    [Input(\"url\", \"pathname\")]\n)\ndef toggle_active_links(pathname):\n    if pathname == \"/\":\n        # Treat page 1 as the homepage / index\n        return True, False, False\n    return [pathname == f\"/page-{i}\" for i in range(1, 4)]\n\n\n@app.callback(Output('intermediate-value', 'data'), Input('dropdown-models', 'value'))\ndef make_calculations(value):\n    if value is not None:\n        incoming_batch_results, test_results, psi_table, psi_var_table_sum, psi_var_table_det, monitoring, scorecard, y_test, pd_X_test, stat_tests_report = helper_functions.calculate_data()\n    else:\n        # incoming_batch_results, test_results, psi_table, psi_var_table_sum, psi_var_table_det, y_test, pd_X_test = helper_functions.initialize_dash_vars()\n        incoming_batch_results = {'binomial': 'N/A',\n                                  'brier': 'N/A',\n                                  'herfindahl': 'N/A',\n                                  'hosmer': 'N/A',\n                                  'spiegelhalter': 'N/A',\n                                  'jeffreys': 'N/A',\n                                  'roc_auc': 'N/A',\n                                  'ber': 'N/A',\n                                  'log_loss': 'N/A',\n                                  'ks': 'N/A'}\n        test_results = {'binomial': 'N/A',\n                        'brier': 'N/A',\n                        'herfindahl': 'N/A',\n                        'hosmer': 'N/A',\n                        'spiegelhalter': 'N/A',\n                        'jeffreys': 'N/A',\n                        'roc_auc': 'N/A',\n                        'ber': 'N/A',\n                        'log_loss': 'N/A',\n                        'ks': 'N/A'}\n        psi_table = None\n        psi_var_table_sum = None\n        psi_var_table_det = None\n        y_test = None\n        pd_X_test = None\n        stat_tests_report = None\n\n    incoming_batch_results = json.dumps(incoming_batch_results)\n    test_results = json.dumps(test_results)\n\n    if isinstance(psi_table, pd.DataFrame):\n        psi_table = psi_table.to_json(date_format='iso', orient='split')\n    else:\n        psi_table = json.dumps(psi_table)\n\n    if isinstance(psi_var_table_sum, pd.DataFrame):\n        psi_var_table_sum = psi_var_table_sum.to_json(date_format='iso', orient='split')\n    else:\n        psi_var_table_sum = json.dumps(psi_var_table_sum)\n\n    if isinstance(psi_var_table_det, pd.DataFrame):\n        psi_var_table_det = psi_var_table_det.to_json(date_format='iso', orient='split')\n    else:\n        psi_var_table_det = json.dumps(psi_var_table_det)\n\n    if y_test is None:\n        y_test = json.dumps(y_test)\n    else:\n        y_test = json.dumps(y_test.tolist())\n\n    if pd_X_test is None:\n        pd_X_test = json.dumps(pd_X_test)\n    else:\n        pd_X_test = json.dumps(pd_X_test.tolist())\n\n    stat_tests_report = json.dumps(stat_tests_report)\n\n    datasets = {'incoming_batch_results': incoming_batch_results,\n                'test_results': test_results,\n                'psi_table': psi_table,\n                'psi_var_table_sum': psi_var_table_sum,\n                'psi_var_table_det': psi_var_table_det,\n                'y_test': y_test,\n                'pd_X_test': pd_X_test,\n                'stat_tests_report': stat_tests_report}\n    return json.dumps(datasets)\n\n\n@app.callback(\n    Output(\"page-content\", \"children\"),\n    Input(\"url\", \"pathname\"),\n    Input(\"intermediate-value\", 'data')\n)\ndef render_page_content(pathname, data):\n    datasets = json.loads(data)\n\n    incoming_batch_results = json.loads(datasets['incoming_batch_results'])\n    test_results = json.loads(datasets['test_results'])\n\n    if datasets['psi_table'] == 'null':\n        psi_table = json.loads(datasets['psi_table'])\n    else:\n        psi_table = pd.read_json(datasets['psi_table'], orient='split')\n\n    if datasets['psi_var_table_sum'] == 'null':\n        psi_var_table_sum = json.loads(datasets['psi_var_table_sum'])\n    else:\n        psi_var_table_sum = pd.read_json(datasets['psi_var_table_sum'], orient='split')\n\n    if datasets['psi_var_table_det'] == 'null':\n        psi_var_table_det = json.loads(datasets['psi_var_table_det'])\n    else:\n        psi_var_table_det = pd.read_json(datasets['psi_var_table_det'], orient='split')\n\n    y_test = json.loads(datasets['y_test'])\n    pd_X_test = json.loads(datasets['pd_X_test'])\n    stat_tests_report = json.loads(datasets['stat_tests_report'])\n\n    if pathname in [\"/\", \"/page-1\"]:\n        page_content = pages.accuracy.generate_accuracy_page(incoming_batch_results, test_results, y_test, pd_X_test)\n        return page_content\n\n    elif pathname == \"/page-2\":\n        page_content = pages.stability.generate_stability_page(incoming_batch_results, test_results, psi_table,\n                                                               psi_var_table_sum)\n\n        return page_content\n    elif pathname == \"/page-3\":\n        page_content = html.H4(stat_tests_report)\n        return page_content\n\n    elif pathname == \"/summary\":\n        page_content = html.Div([\n            html.H2('Overall quality score: GOOD'),\n            html.H3('AUC')\n        ])\n        return page_content\n\n    elif pathname == '/report-generator':\n        page_content = html.Div([\n\n            html.H2('Generate report here')\n\n        ])\n        return page_content\n\n    return dbc.Container(\n        [\n            html.H1(\"404: Not found\", className=\"text-danger\"),\n            html.Hr(),\n            html.P(f\"The pathname {pathname} was not recognised...\"),\n        ]\n    )\n\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=8086)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# styles.py\ndef get_sidebar_style():\n    SIDEBAR_STYLE = {\n        \"position\": \"fixed\",\n        \"top\": 75.0,\n        \"left\": 0,\n        \"bottom\": 0,\n        \"width\": \"16rem\",\n        \"height\": \"100%\",\n        \"z-index\": 1,\n        \"overflow-x\": \"hidden\",\n        \"transition\": \"all 0.5s\",\n        \"padding\": \"0.5rem 1rem\",\n        \"background-color\": \"#111111\",\n    }\n    return SIDEBAR_STYLE\n\ndef get_sidebar_hidden_style():\n    SIDEBAR_HIDDEN = {\n        \"position\": \"fixed\",\n        \"top\": 62.5,\n        \"left\": \"-16rem\",\n        \"bottom\": 0,\n        \"width\": \"16rem\",\n        \"height\": \"100%\",\n        \"z-index\": 1,\n        \"overflow-x\": \"hidden\",\n        \"transition\": \"all 0.5s\",\n        \"padding\": \"0rem 0rem\",\n        \"background-color\": \"#111111\",\n    }\n    return SIDEBAR_HIDDEN\n\ndef get_content_style():\n    # the styles for the main content position it to the right of the sidebar and\n    # add some padding.\n    CONTENT_STYLE = {\n        \"transition\": \"margin-left .5s\",\n        \"margin-left\": \"18rem\",\n        \"margin-right\": \"2rem\",\n        \"padding\": \"2rem 1rem\",\n        \"background-color\": \"#111111\",\n    }\n\n    CONTENT_STYLE1 = {\n        \"transition\": \"margin-left .5s\",\n        \"margin-left\": \"2rem\",\n        \"margin-right\": \"2rem\",\n        \"padding\": \"2rem 1rem\",\n        \"background-color\": \"#111111\",\n    }\n    return CONTENT_STYLE, CONTENT_STYLE1\n\ndef get_footer_style():\n    FOOTER_STYLE = {\n        \"position\": \"absolute\",\n        \"bottom\": \"0\",\n        \"width\": \"100%\",\n        \"height\": \"60px\",  # /* Set the fixed height of the footer here */\n        \"line-height\": \"60px\",  # /* Vertically center the text there */\n        \"background-color\": \"#111111\"\n    }\n    return FOOTER_STYLE\n\ndef get_modal_style():\n    MODAL = {\n        \"position\": \"fixed\",\n        \"z-index\": \"1002\",  # /* Sit on top, including modebar which has z=1001 */\n        \"left\": \"0\",\n        \"top\": \"0\",\n        \"width\": \"100%\",  # /* Full width */\n        \"height\": \"100%\",  # /* Full height */\n        \"background-color\": \"rgba(0, 0, 0, 0.6)\",\n        \"display\": \"block\"  # /* Black w/ opacity */\n    }\n    return MODAL","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy.py\nfrom dash import dcc, html\nimport dash_bootstrap_components as dbc\nfrom plotting_functions import new_create_card, plot_auc_roc, plot_ks, no_figure_plot\n\n\ndef generate_accuracy_page(incoming_batch_results, test_results, y_test, pd_X_test):\n    page_content = html.Div([\n        dbc.Row(\n            [\n                dbc.Col(\n                    new_create_card('Log Loss', '12412', incoming_batch_results['log_loss'],\n                                    test_results['log_loss'])\n                ),\n\n                dbc.Col(\n                    new_create_card('Brier', '12413', incoming_batch_results['brier'],\n                                    test_results['brier'])\n                ),\n\n                dbc.Col(\n                    new_create_card('K-S', '12414', incoming_batch_results['ks'],\n                                    test_results['ks'])\n                ),\n\n                dbc.Col(\n                    new_create_card('AUROC', '12415', incoming_batch_results['roc_auc'],\n                                    test_results['roc_auc'])\n                ),\n\n                dbc.Col(\n                    new_create_card('BER', '12416', incoming_batch_results['ber'],\n                                    test_results['ber'])\n                )\n            ]),\n        # dbc.Row([html.Div(dcc.Dropdown(X_test.columns, 'age', id='dropdown'),\n        #                   style={'width': '50%'}\n        #                   ),\n        #          html.Div(dcc.Dropdown(X_test.columns, 'age', id='dropdown_2'),\n        #                   style={'width': '50%'}\n        #                   )\n        #          ]\n        #         ),\n        dbc.Row([\n\n            dbc.Col(\n                dcc.Graph(id='logloss-plot',\n                          figure=no_figure_plot()\n                          ),\n                width=5\n            ),\n            dbc.Col(\n                dcc.Graph(id='graph-with-slider',\n                          figure=plot_auc_roc(y_test, pd_X_test)\n                          ),\n                width=3\n            ),\n            dbc.Col(\n                dcc.Graph(id='graph-distributions',\n                          figure=plot_ks(y_test, pd_X_test)\n                          ),\n                width=3\n            )\n        ])\n    ])\n    return page_content\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting_functions.py\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport dash_bootstrap_components as dbc\nfrom dash import Dash, dcc, html\nimport numpy as np\nimport pandas as pd\nimport plotly.io as pio\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\npio.templates.default = \"plotly_dark\"\n\n\ndef no_figure_plot():\n    return go.Figure().add_annotation(x=2, y=2, text=\"No Data\",\n                                      font=dict(family=\"sans serif\", size=25, color=\"crimson\"),\n                                      showarrow=False, yshift=10)\n\n\ndef psi_single_variable_plot(psi_var_table):\n    # psi_table = monitoring.psi_variable_table(style=style)\n    fig = go.Figure()\n    if style == 'detailed':\n        var_table = psi_var_table[psi_var_table['Variable'] == var]\n        fig.add_trace(go.Bar(x=var_table.index, y=var_table['Count A (%)']))\n        fig.add_trace(go.Bar(x=var_table.index, y=var_table['Count E (%)']))\n    return fig\n\n\ndef psi_variable_plot(psi_variable_table):\n    if psi_variable_table is None:\n        return no_figure_plot()\n    # CHECK STYLE ONLY SUMMARY OR DETAILED\n    fig = go.Figure()\n    # if style == 'summary':\n    fig.add_trace(go.Bar(x=psi_variable_table['Variable'], y=psi_variable_table['PSI']))\n    # elif style == 'detailed':\n    #     fig = go.Figure()\n    #     fig\n    return fig\n\n\n### TAKE DATA FROM monitoring.psi_table(). same data as here\ndef psi_plot_ly(psi_table):\n    if psi_table is None:\n        return no_figure_plot()\n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=psi_table.index, y=psi_table['Count A (%)']))\n    fig.add_trace(go.Bar(x=psi_table.index, y=psi_table['Count E (%)']))\n    return fig\n\n\n# def psi_plot_ly(psi_table):\n#     fig = go.Figure()\n#     fig.add_trace(go.Bar(x=psi_table.index, y=psi_table['Count A (%)']))\n#     fig.add_trace(go.Bar(x=psi_table.index, y=psi_table['Count E (%)']))\n#     return fig\n\n\ndef plot_ks(y, y_pred, title=None, xlabel=None, ylabel=None):\n    if y is None or y_pred is None:\n        return go.Figure().add_annotation(x=2, y=2, text=\"No Data to Display\",\n                                          font=dict(family=\"sans serif\", size=25, color=\"crimson\"),\n                                          showarrow=False, yshift=10)\n\n\n    import plotly.express as px\n\n    # pio.templates.default = \"plotly_dark\"\n\n    y = pd.Series(y)\n    y_pred = pd.Series(y_pred)\n    n_samples = y.shape[0]\n    # n_samples = len(y)\n    n_event = np.sum(y)\n    n_nonevent = n_samples - n_event\n\n    idx = y_pred.argsort()\n    yy = y[idx]\n    pp = y_pred[idx]\n\n    cum_event = np.cumsum(yy)\n    cum_population = np.arange(0, n_samples)\n    cum_nonevent = cum_population - cum_event\n\n    p_event = cum_event / n_event\n    p_nonevent = cum_nonevent / n_nonevent\n\n    p_diff = p_nonevent - p_event\n\n    ks_score = np.max(p_diff)\n    ks_max_idx = np.argmax(p_diff)\n    # Define the plot settings\n    print('plot')\n    print(p_diff)\n    if title is None:\n        title = \"Kolmogorov-Smirnov\"\n    if xlabel is None:\n        xlabel = \"Threshold\"\n    if ylabel is None:\n        ylabel = \"Cumulative probability\"\n\n    # plt.title(title, fontdict={'fontsize': 14})\n    # plt.xlabel(xlabel, fontdict={'fontsize': 12})\n    # plt.ylabel(ylabel, fontdict={'fontsize': 12})\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=pp, y=p_event))\n    fig.add_trace(go.Scatter(x=pp, y=p_nonevent))\n    return fig\n\n\ndef plot_auc_roc(y, y_pred, title=None, xlabel=None, ylabel=None):\n    # savefig = False, fname = None, ** kwargs)\n    if y is None or y_pred is None:\n        return no_figure_plot()\n\n\n\n\n    print('THIS IS Y: ', y[0:10])\n    print('THIS IS Y_PRED: ', y_pred[0:10])\n    fpr, tpr, _ = roc_curve(y, y_pred)\n    auc_roc = roc_auc_score(y, y_pred)\n\n    # # Define the plot settings\n    # if title is None:\n    #     title = \"ROC curve\"\n    # if xlabel is None:\n    #     xlabel = \"False Positive Rate\"\n    # if ylabel is None:\n    #     ylabel = \"True Positive Rate\"\n\n    layout = dict(\n        title='ROC-AUC',\n        xaxis_title='False Positive Rate',\n        yaxis_title='True Positive Rate',\n        legend_title='Legend Title',\n        font=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='red'\n        )\n    )\n\n    fig = go.Figure(layout=layout)\n    fig.add_trace(go.Scatter(x=fpr, y=fpr, name='Stupid model'))\n    fig.add_trace(go.Scatter(x=fpr, y=tpr, name='Your model'))\n\n\n    return fig\n\n\ndef plot_hist_dist(factor1, factor2):\n    fig = go.Figure()\n    fig.add_trace(go.Histogram(x=factor1, histnorm='probability density'))\n    fig.add_trace(go.Histogram(x=factor2, histnorm='probability density', opacity=0.5))\n\n    fig.update_layout(barmode='overlay')\n    # Reduce opacity to see both histograms\n    # fig.update_traces(opacity=0.3)\n    return fig\n\n\n# Plots feature importances\ndef plot_feature_importances(feat_imp_df):\n    fig = px.bar(feat_imp_df.sort_values(['Importance']), x=\"Importance\", y=\"Feature\", orientation='h')\n    fig.update_layout(xaxis=dict(rangeslider=dict(visible=True),\n                                 type=\"linear\"))\n    return fig\n\n\n# Creates card with information on the dashboard\ndef create_card(value, card_id, title, description):\n    return dbc.Card(\n        dbc.CardBody(\n            [\n                html.H4(title, id=f\"{card_id}-title\"),\n                html.H2(f'{value:.3f}', id=f\"{card_id}-value\"),\n                html.P(description, id=f\"{card_id}-description\")\n            ]\n        ),\n        body=True,\n        color='dark',\n        outline=True\n    )\n\n\ndef new_create_card(title, card_id, new_value, prev_value, description=None):\n    def f(v):\n        if v is None:\n            return v\n        elif v == 'N/A':\n            return v\n        return f\"{v:.3f}\"\n\n    card = dbc.Card(\n        dbc.CardBody(\n            [\n                html.H4(title, id=f\"{card_id}-title\"),\n                html.H2(f(new_value), id=f\"{card_id}-value\"),\n                html.H6(f'Initial: {f(prev_value)}', id=f\"{card_id}-prev-value\"),\n                html.P(description, id=f\"{card_id}-description\")\n            ]\n        ),\n        body=True,\n        color='dark',\n        outline=True\n    )\n    return card","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mpp.py\n# Model Performance Predictor\n# predicts the model's metrics\n\nclass ModelPerformancePredictor:\n    def __init__(self, x_test=None, y_test=None):\n        self.x_test = x_test\n        self.y_test = y_test\n\n\n    def fit(self):\n\n    def _fit(self):\n\n    def predict(self):\n\n    def _predict(self):\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper_functions.py\nfrom optbinning.binning import binning_process\nfrom optbinning.scorecard.scorecard import _check_parameters\nfrom optbinning.scorecard.scorecard import _compute_scorecard_points\nfrom sklearn.base import TransformerMixin\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport credit_py_validation as cpv\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom optbinning.scorecard import Scorecard\nfrom optbinning.logging import Logger\n\nimport time\nfrom sklearn.base import clone\nfrom sklearn.utils.multiclass import type_of_target\nimport gitlab\nimport sqlite3\nimport json\nimport io\nfrom contextlib import redirect_stdout\n\nfrom optbinning import BinningProcess\nfrom optbinning.scorecard import ScorecardMonitoring\n\nlogger = Logger(__name__).logger\n\n\ndef capture_output(output_stream):\n    f = io.StringIO()\n    with redirect_stdout(f):\n        output_stream\n        # output_stream.system_stability_report()\n    output = f.getvalue()\n    return output\n\n\ndef get_all_models():\n    gl = gitlab.Gitlab('http://gitlab.cloud.halykbank.nb', private_token='yBKJTSeuciukPxTfWJ-Z',\n                       ssl_verify=False, api_version=4)\n    project = gl.projects.get(2370)\n    repo = project.repository_tree(ref='master')\n    directories = [item[\"path\"] for item in repo if item[\"type\"] == \"tree\"]\n    return directories\n\n\ndef conduct_tests(table_for_tests):  # __REWORK - add options to choose model and different tests\n    # if self.table_for_tests is None:\n    #     raise Exception('Dataset for tests is not ready')\n    binomial_result = cpv.binomial_test(table_for_tests, 'Credit Rating', 'Default Flag',\n                                        'Default Probability').to_dict()\n    brier_score_result = cpv.brier_score(table_for_tests, 'Credit Rating', 'Default Flag',\n                                         'Default Probability')\n    herfindahl_result = cpv.herfindahl_test(table_for_tests, 'Credit Rating')\n    hosmer_result = cpv.hosmer_test(table_for_tests, 'Credit Rating', 'Default Flag', 'Default Probability')\n    spiegelhalter_result = cpv.spiegelhalter_test(table_for_tests, 'Credit Rating', 'Default Flag',\n                                                  'Default Probability')\n    jeffreys_result = cpv.jeffreys_test(table_for_tests, 'Credit Rating', 'Default Flag',\n                                        'Default Probability').to_dict()\n    roc_auc_result = cpv.roc_auc(table_for_tests, 'Default Flag', 'Default Probability')  # FIX - outputs 1.0 AUC\n    ber_result = cpv.bayesian_error_rate(table_for_tests['Default Flag'],\n                                         table_for_tests['Default Probability'])\n    log_loss_result = log_loss(table_for_tests['Default Flag'], table_for_tests['Default Probability'])\n\n    ks_result = cpv.kolmogorov_smirnov(table_for_tests['Default Flag'].reset_index(drop=True),\n                                       table_for_tests['Default Probability'].reset_index(drop=True))\n    # cpv.psi(loans_test, 'Credit Rating', 'PD')\n\n    test_results = {'binomial': binomial_result,\n                    'brier': brier_score_result,\n                    'herfindahl': herfindahl_result,\n                    'hosmer': hosmer_result,\n                    'spiegelhalter': spiegelhalter_result,\n                    'jeffreys': jeffreys_result,\n                    'roc_auc': roc_auc_result,\n                    'ber': ber_result,\n                    'log_loss': log_loss_result,\n                    'ks': ks_result}\n    return test_results\n\n\ndef initialize_dash_vars():\n    init_cols = ['Bin', 'Count A', 'Count E', 'Count A (%)', 'Count E (%)', 'PSI']\n    stat_test_results = {'binomial': 'N/A',\n                         'brier': 'N/A',\n                         'herfindahl': 'N/A',\n                         'hosmer': 'N/A',\n                         'spiegelhalter': 'N/A',\n                         'jeffreys': 'N/A',\n                         'roc_auc': 'N/A',\n                         'ber': 'N/A',\n                         'log_loss': 'N/A'}\n    psi_table = pd.DataFrame(columns=init_cols)\n    psi_var_table_sum = pd.DataFrame(columns=init_cols)\n    psi_var_table_det = pd.DataFrame(columns=init_cols)\n    y_test = pd.Series()\n    pd_X_test = pd.Series()\n    return stat_test_results, stat_test_results, psi_table, psi_var_table_sum, psi_var_table_det, y_test, pd_X_test\n\n\ndef load_full_simulation_df():  # TEMPORARY FUNCTION FOR SIMULATION\n    con = sqlite3.connect('new_credit_data.db')\n    query_X = \"\"\"SELECT *\n    FROM X_train_log\n    \"\"\"\n    query_y = \"\"\"SELECT *\n    FROM y_train_log\n    \"\"\"\n    X = pd.read_sql(query_X, con, index_col=None)\n    y = pd.read_sql(query_y, con, index_col=None)\n    y = y.values.ravel()\n    X_train, X_unseen, y_train, y_unseen = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, X_unseen, y_train, y_unseen\n\n\ndef load_model():\n    with open('binning_fit_params.json') as json_file:\n        binning_fit_params = json.load(json_file)\n    # Loading the model\n    lr_model = joblib.load('lr_model.pkl')\n    return lr_model, binning_fit_params\n\n\ndef calculate_data():  # RE -FUCKING- DO IT\n    X_train, X_unseen, y_train, y_unseen = load_full_simulation_df()\n    lr_model, binning_fit_params = load_model()\n\n    variable_names = list(X_train.columns)\n    binning_process = BinningProcess(variable_names,  # selection_criteria=selection_criteria,\n                                     binning_fit_params=binning_fit_params)\n    scorecard = AutoPrepareScoreCard(binning_process=binning_process,\n                                     estimator=lr_model, scaling_method=\"min_max\",\n                                     scaling_method_params={\"min\": 300, \"max\": 850}, verbose=True)\n\n    scorecard.fit(X_train, y_train)\n    monitoring = ScorecardMonitoring(scorecard=scorecard, psi_method=\"cart\",\n                                     psi_n_bins=10, verbose=True)\n\n    ff = io.StringIO()\n    with redirect_stdout(ff):\n        monitoring.fit(X_unseen, y_unseen, X_train, y_train)\n    asd2 = ff.getvalue()\n\n    stat_tests_report = capture_output(monitoring.system_stability_report())\n\n    # Calculate train\n    pd_X_train = scorecard.predict_proba(X_train)[:, 1]\n    default_flag_X_train = scorecard.predict(X_train)\n    score_X_train = scorecard.score(X_train)\n    rating_X_train = scorecard.get_credit_ratings(score_X_train)\n\n    X_train['Default Probability'] = pd_X_train\n    X_train['Credit Rating'] = rating_X_train\n    X_train['Default Flag'] = default_flag_X_train\n\n    # Calculate test\n    pd_X_test = scorecard.predict_proba(X_unseen)[:, 1]\n    default_flag_X_test = scorecard.predict(X_unseen)\n    score_X_test = scorecard.score(X_unseen)\n    rating_X_test = scorecard.get_credit_ratings(score_X_test)\n\n    print('pd_X_Test: ', pd_X_test)\n    print('def_X_test: ', default_flag_X_test)\n    unique, counts = np.unique(default_flag_X_test, return_counts=True)\n    print(np.asarray((unique, counts)).T)\n\n    X_unseen['Default Probability'] = pd_X_test\n    X_unseen['Credit Rating'] = rating_X_test\n    X_unseen['Default Flag'] = default_flag_X_test\n\n    incoming_batch_results = conduct_tests(X_unseen)\n    test_results = conduct_tests(X_train)\n    psi_var_table_det = monitoring.psi_variable_table(style='detailed')\n    psi_var_table_sum = monitoring.psi_variable_table(style='summary')\n    psi_table = monitoring.psi_table()\n\n    return incoming_batch_results, test_results, psi_table, psi_var_table_sum, psi_var_table_det, monitoring, scorecard, y_unseen, pd_X_test, stat_tests_report\n\n\n# CHANGE ALL\nclass AutoPrepareScoreCard(Scorecard):\n    def __init__(self, binning_process, estimator, scaling_method=None, scaling_method_params=None,\n                 intercept_based=False, reverse_scorecard=False, rounding=False, verbose=False, db_connection=None,\n                 target_name='target'):  # ADD PARAMETERS\n        super().__init__(binning_process, estimator, scaling_method=None, scaling_method_params=None,\n                         intercept_based=False, reverse_scorecard=False, rounding=False, verbose=False)\n\n        self.binning_process = binning_process\n        self.estimator = estimator\n        self.scaling_method = scaling_method\n        self.scaling_method_params = scaling_method_params\n        self.intercept_based = intercept_based\n        self.reverse_scorecard = reverse_scorecard\n        self.rounding = rounding\n        self.verbose = verbose\n\n        self.incoming_labels = None\n        self.test_data_tests = None\n        self.incoming_data_tests = None\n        self.incoming_data = None\n        self.db_connection = db_connection\n        self.target_name = target_name\n\n        self.train_set = None\n        self.test_set = None\n        self.pipeline = None\n\n        self.test_predictions = None\n\n        self.choices = list(reversed(range(1, 11)))\n        self.loan_type = 'УЗП'\n\n        self.credit_scores = None\n        self.credit_ratings = None\n\n        # attributes\n        self.binning_process_ = None\n        self.estimator_ = None\n        self.intercept_ = 0\n\n        self._metric_special = None\n        self._metric_missing = None\n\n        # auxiliary\n        self._target_dtype = None\n\n        # timing\n        self._time_total = None\n        self._time_binning_process = None\n        self._time_estimator = None\n        self._time_build_scorecard = None\n        self._time_rounding = None\n\n        self._is_fitted = False\n\n    def fit(self, X, y, sample_weight=None, metric_special=0, metric_missing=0,\n            show_digits=2, check_input=False, fitted=True):\n        \"\"\"Fit scorecard.\n        Parameters\n        ----------\n        X : pandas.DataFrame (n_samples, n_features)\n            Training vector, where n_samples is the number of samples.\n        y : array-like of shape (n_samples,)\n            Target vector relative to x.\n        sample_weight : array-like of shape (n_samples,) (default=None)\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n            This option is only available for a binary target.\n        metric_special : float or str (default=0)\n            The metric value to transform special codes in the input vector.\n            Supported metrics are \"empirical\" to use the empirical WoE or\n            event rate, and any numerical value.\n        metric_missing : float or str (default=0)\n            The metric value to transform missing values in the input vector.\n            Supported metrics are \"empirical\" to use the empirical WoE or\n            event rate and any numerical value.\n        check_input : bool (default=False)\n            Whether to check input arrays.\n        show_digits : int, optional (default=2)\n            The number of significant digits of the bin column.\n        fitted : boolean, optional (default=True)\n            If the estimator is fitted or not\n        Returns\n        -------\n        self : Scorecard\n            Fitted scorecard.\n        \"\"\"\n        return self._fit(X, y, sample_weight, metric_special, metric_missing,\n                         show_digits, check_input, fitted)\n\n    def _fit(self, X, y, sample_weight, metric_special, metric_missing,\n             show_digits, check_input, fitted=True):\n\n        # Store the metrics for missing and special bins for predictions\n        self._metric_special = metric_special\n        self._metric_missing = metric_missing\n\n        time_init = time.perf_counter()\n\n        # if self.verbose:\n        #     logger.info(\"Scorecard building process started.\")\n        #     logger.info(\"Options: check parameters.\")\n\n        # _check_parameters(**self.get_params(deep=False))\n\n        # Check X dtype\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"X must be a pandas.DataFrame.\")\n\n        # Target type and metric\n        self._target_dtype = type_of_target(y)\n\n        if self._target_dtype not in (\"binary\", \"continuous\"):\n            raise ValueError(\"Target type {} is not supported.\"\n                             .format(self._target_dtype))\n\n        # _check_scorecard_scaling(self.scaling_method,\n        #                          self.scaling_method_params,\n        #                          self.rounding,\n        #                          self._target_dtype)\n\n        # # Check sample weight\n        # if sample_weight is not None and self._target_dtype != \"binary\":\n        #     raise ValueError(\"Target type {} does not support sample weight.\"\n        #                      .format(self._target_dtype))\n        metric = 'woe'\n        bt_metric = 'WoE'\n        if self._target_dtype == \"binary\":\n            metric = \"woe\"\n            bt_metric = \"WoE\"\n        elif self._target_dtype == \"continuous\":\n            metric = \"mean\"\n            bt_metric = \"Mean\"\n        # if self.verbose:\n        #     logger.info(\"Dataset: {} target.\".format(self._target_dtype))\n\n        # Fit binning process\n        if self.verbose:\n            logger.info(\"Binning process started.\")\n\n        time_binning_process = time.perf_counter()\n        self.binning_process_ = clone(self.binning_process)\n\n        # Suppress binning process verbosity\n        self.binning_process_.set_params(verbose=False)\n\n        X_t = self.binning_process_.fit_transform(\n            X[self.binning_process.variable_names], y, sample_weight, metric,\n            metric_special, metric_missing, show_digits, check_input)\n\n        self._time_binning_process = time.perf_counter() - time_binning_process\n\n        # if self.verbose:\n        #     logger.info(\"Binning process terminated. Time: {:.4f}s\"\n        #                 .format(self._time_binning_process))\n\n        if not fitted:\n            # Fit estimator\n            time_estimator = time.perf_counter()\n            if self.verbose:\n                logger.info(\"Fitting estimator.\")\n\n            self.estimator_ = clone(self.estimator)\n            self.estimator_.fit(X_t, y, sample_weight)\n\n            self._time_estimator = time.perf_counter() - time_estimator\n\n            if self.verbose:\n                logger.info(\"Fitting terminated. Time {:.4f}s\"\n                            .format(self._time_estimator))\n        else:\n            from copy import deepcopy\n            time_estimator = time.perf_counter()\n            self.estimator_ = deepcopy(self.estimator)\n            self._time_estimator = time.perf_counter() - time_estimator\n\n        # Get coefs\n        intercept = 0\n        if hasattr(self.estimator_, 'coef_'):\n            coefs = self.estimator_.coef_.flatten()\n            if hasattr(self.estimator_, 'intercept_'):\n                intercept = self.estimator_.intercept_\n        else:\n            raise RuntimeError('The classifier does not expose '\n                               '\"coef_\" attribute.')\n\n        # Build scorecard\n        time_build_scorecard = time.perf_counter()\n\n        if self.verbose:\n            logger.info(\"Scorecard table building started.\")\n\n        selected_variables = self.binning_process_.get_support(names=True)\n        binning_tables = []\n        for i, variable in enumerate(selected_variables):\n            optb = self.binning_process_.get_binned_variable(variable)\n            binning_table = optb.binning_table.build(\n                show_digits=show_digits, add_totals=False)\n\n            c = coefs[i]\n            binning_table.loc[:, \"Variable\"] = variable\n            binning_table.loc[:, \"Coefficient\"] = c\n            binning_table.loc[:, \"Points\"] = binning_table[bt_metric] * c\n\n            nt = len(binning_table)\n            if metric_special != 'empirical':\n                if isinstance(optb.special_codes, dict):\n                    n_specials = len(optb.special_codes)\n                else:\n                    n_specials = 1\n\n                binning_table.loc[\n                nt - 1 - n_specials:nt - 2, \"Points\"] = metric_special * c\n            elif metric_missing != 'empirical':\n                binning_table.loc[nt - 1, \"Points\"] = metric_missing * c\n\n            binning_table.index.names = ['Bin id']\n            binning_table.reset_index(level=0, inplace=True)\n            binning_tables.append(binning_table)\n\n        df_scorecard = pd.concat(binning_tables)\n        df_scorecard.reset_index()\n\n        # Apply score points\n        if self.scaling_method is not None:\n            points = df_scorecard[\"Points\"]\n            scaled_points = _compute_scorecard_points(\n                points, binning_tables, self.scaling_method,\n                self.scaling_method_params, intercept, self.reverse_scorecard)\n\n            df_scorecard.loc[:, \"Points\"] = scaled_points\n\n        if self.intercept_based:\n            scaled_points, self.intercept_ = _compute_intercept_based(\n                df_scorecard)\n            df_scorecard.loc[:, \"Points\"] = scaled_points\n\n        time_rounding = time.perf_counter()\n        if self.rounding:\n            points = df_scorecard[\"Points\"]\n            if self.scaling_method in (\"pdo_odds\", None):\n                round_points = np.rint(points)\n\n                if self.intercept_based:\n                    self.intercept_ = np.rint(self.intercept_)\n            elif self.scaling_method == \"min_max\":\n                round_mip = RoundingMIP()\n                round_mip.build_model(df_scorecard)\n                status, round_points = round_mip.solve()\n\n                if status not in (\"OPTIMAL\", \"FEASIBLE\"):\n                    if self.verbose:\n                        logger.warning(\"MIP rounding failed, method nearest \"\n                                       \"integer used instead.\")\n                    # Back-up method\n                    round_points = np.rint(points)\n\n                if self.intercept_based:\n                    self.intercept_ = np.rint(self.intercept_)\n\n            df_scorecard.loc[:, \"Points\"] = round_points\n        self._time_rounding = time.perf_counter() - time_rounding\n\n        self._df_scorecard = df_scorecard\n\n        self._time_build_scorecard = time.perf_counter() - time_build_scorecard\n        self._time_total = time.perf_counter() - time_init\n\n        if self.verbose:\n            logger.info(\"Scorecard table terminated. Time: {:.4f}s\"\n                        .format(self._time_build_scorecard))\n            logger.info(\"Scorecard building process terminated. Time: {:.4f}s\"\n                        .format(self._time_total))\n\n        # Completed successfully\n        self._is_fitted = True\n\n        return self\n\n    def read_pipeline(self, path, from_file=True, from_db=False):  # __REDO - for different model formats\n        if from_file:\n            self.pipeline = joblib.load(path)\n            return self.pipeline\n        return None\n\n    def _read_data_db(self, query):\n        return pd.read_sql(query, self.db_connection, index_col=None)\n\n    def read_data(self, train_query, test_query):\n        self.train_set = self._read_data_db(train_query)\n        self.test_set = self._read_data_db(test_query)\n        return self.train_set, self.test_set\n\n    def read_incoming_data(self, incoming_query, incoming_labels_query=None):\n        self.incoming_data = self._read_data_db(incoming_query)\n        if incoming_labels_query is not None:\n            self.incoming_labels = self._read_data_db(incoming_labels_query)\n            self.incoming_data[self.target_name] = self._read_data_db(incoming_labels_query)[self.target_name].copy()\n        return self.incoming_data\n\n    def make_predictions(self, df_set, proba=True):\n        if self.pipeline is None:\n            raise Exception('No pipeline is set to predict')\n        if proba:\n            return self.pipeline.predict_proba(df_set.drop(self.target_name, axis=1, errors='ignore'))[:, 1]\n        return self.pipeline.predict(df_set.drop(self.target_name, axis=1, errors='ignore'))\n\n    def set_score_card(self, choices, loan_type):  # __REWORK - add actual scorecard\n        self.choices = choices\n        self.loan_type = loan_type\n\n    def get_credit_scores(self, df):  # __REWORK - add scoring card construction options (can be non-linear)\n        if df is None:\n            raise Exception('Data set is not specified')\n        default_probs = self.make_predictions(df, proba=True)\n        minmax_sc = MinMaxScaler(feature_range=(300, 850))\n        minmax_sc.fit(default_probs.reshape(-1, 1))\n        credit_scores = minmax_sc.transform(default_probs.reshape(-1, 1)).ravel().round()\n        self.credit_scores = credit_scores\n        return credit_scores\n\n    def get_credit_ratings(self, credit_scores):\n        if self.choices is None:\n            raise Exception('Need to set score card first')\n        if self.loan_type is None:\n            raise Exception('Please specify loan type first')\n        choices = self.choices.copy()\n        if not isinstance(credit_scores, pd.Series):\n            credit_scores = pd.Series(credit_scores)\n        if self.loan_type == 'УЗП' or self.loan_type == 'НЗП':\n            cond_list = [\n                credit_scores.lt(576),\n                credit_scores.between(576, 585),\n                credit_scores.between(586, 594),\n                credit_scores.between(595, 603),\n                credit_scores.between(604, 610),\n                credit_scores.between(611, 617),\n                credit_scores.between(618, 625),\n                credit_scores.between(626, 633),\n                credit_scores.between(634, 642),\n                credit_scores.gt(642)]\n        #     else if loan_type == 'Пенсионная':\n        #         cond_list = [\n        #             df_series.lt(1),\n        #             df_series.between(1, 30),\n        #             df_series.between(31, 60),\n        #             df_series.between(61, 90),\n        #             df_series.between(91, 180),\n        #             df_series.gt(180)]\n        credit_ratings = np.select(cond_list, choices)\n\n        self.credit_ratings = credit_ratings\n        return credit_ratings\n\n    def prepare_test_tests(self):\n        if self.test_set is None:\n            raise Exception('Test set is missing')\n        else:\n            test_data_tests = self.test_set.copy()\n        test_data_tests['Credit Rating'] = self.credit_ratings.copy()\n        test_data_tests.rename(columns={self.target_name: 'Default Flag'}, inplace=True)\n        test_data_tests['Default Probability'] = self.make_predictions(self.test_set, proba=True)\n        self.test_data_tests = test_data_tests.copy()\n        return test_data_tests\n\n    def prepare_incoming_tests(self):\n        if self.incoming_data is None:\n            raise Exception('Incoming data is missing')\n        else:\n            incoming_data_tests = self.incoming_data.copy()\n        incoming_data_tests['Credit Rating'] = self.credit_ratings.copy()\n        incoming_data_tests.rename(columns={'target': 'Default Flag'}, inplace=True)\n        incoming_data_tests['Default Probability'] = self.make_predictions(self.incoming_data, proba=True)\n        self.incoming_data_tests = incoming_data_tests.copy()\n        return incoming_data_tests\n\n\n# --------------------------------------------------------------------------CUSTOM TRANSFORMERS FOR PICKLE FILE\n\ndef col_dropper(df):\n    df = df.drop(['INCOME_CALC_METHOD', 'SUM_PROCENTY_TODAY_Sum'], axis=1)\n    return df\n\n\ndef fix_dtypes(df):\n    df['USTUPKA_DEISTV'] = df['USTUPKA_DEISTV'].astype(int)\n    df['USTUPKA_ZAKRYTYE'] = df['USTUPKA_ZAKRYTYE'].astype(int)\n    df['GENDER'] = df['GENDER'].astype(int)\n    df['CREDIT_HISTORY'] = df['CREDIT_HISTORY'].astype(int)\n    df['SUBJ_STATUS'] = df['SUBJ_STATUS'].astype(int)\n    df['age'] = df['age'].astype(int)\n    return df\n\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value\n        in column.\n\n        Columns of other types are imputed with mean of column.\n\n        \"\"\"\n\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n                               if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n                              index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n# --------------------------------------------------------------------------CUSTOM TRANSFORMERS FOR PICKLE FILE\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stat_tests_report.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stability.py\nfrom dash import dcc, html\nimport dash_bootstrap_components as dbc\nfrom plotting_functions import new_create_card, psi_plot_ly, psi_variable_plot\n\n\ndef generate_stability_page(incoming_batch_results, test_results, psi_table, psi_var_table_sum):\n    page_content = html.Div([\n        dbc.Row(\n            [\n                dbc.Col(\n                    new_create_card('Log Loss', '12412', incoming_batch_results['log_loss'],\n                                    test_results['log_loss'])\n                ),\n\n                dbc.Col(\n                    new_create_card('Brier', '12413', incoming_batch_results['brier'],\n                                    test_results['brier'])\n                ),\n\n                dbc.Col(\n                    new_create_card('Log Loss', '12414', incoming_batch_results['log_loss'],\n                                    test_results['log_loss'])\n                ),\n\n                dbc.Col(\n                    new_create_card('AUROC', '12415', incoming_batch_results['roc_auc'],\n                                    test_results['roc_auc'])\n                ),\n\n                dbc.Col(\n                    new_create_card('BER', '12416', incoming_batch_results['ber'],\n                                    test_results['ber'])\n                )\n            ]),\n\n        # dbc.Row([html.Div(dcc.Dropdown(X_test.columns, 'age', id='dropdown'),\n        #                   style={'width': '50%'}\n        #                   ),\n        #          html.Div(dcc.Dropdown(X_test.columns, 'age', id='dropdown_2'),\n        #                   style={'width': '50%'}\n        #                   )\n        #          ]\n        #         ),\n\n        dbc.Row([\n            dbc.Col(\n                dcc.Graph(id='graph-with-slider', figure=psi_variable_plot(psi_var_table_sum))\n            ),\n            dbc.Col(\n                dcc.Graph(id='graph-distributions',\n                          figure=psi_plot_ly(psi_table)\n                          )\n            )\n        ])\n    ])\n    return page_content\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# page_content.py\n\nfrom dash import Dash, dcc, html, Input, Output\nimport dash_bootstrap_components as dbc\n\n\ndef generate_search_bar():\n    search_bar = dbc.Row(\n        [\n            dbc.Col(dbc.Input(type=\"search\", placeholder=\"Search\")),\n            dbc.Col(\n                dbc.Button(\n                    \"Search\", color=\"primary\", className=\"ms-2\", n_clicks=0\n                ),\n                width=\"auto\",\n            ),\n        ],\n        className=\"g-0 ms-auto flex-nowrap mt-3 mt-md-0\",\n        align=\"center\",\n    )\n    return search_bar\n\n\ndef generate_navbar(model_list, logo):\n    navbar = dbc.Navbar(\n        dbc.Container(\n            [\n                html.A(\n                    # Use row and col to control vertical alignment of logo / brand\n                    dbc.Row(\n                        [\n                            dbc.Col(html.Img(src=logo, height=\"30px\")),\n                            dbc.Col(dbc.NavbarBrand(\"Credit Risk Model Monitoring System\", className=\"ms-2\")),\n                        ],\n                        align=\"left\",\n                        className=\"g-0\",\n                    ),\n                    href=\"https://halykbank.kz/\",\n                    style={\"textDecoration\": \"none\"},\n                ),\n                dbc.NavbarToggler(id=\"navbar-toggler\", n_clicks=0),\n                html.Div(dcc.Dropdown(model_list, id='dropdown-models', placeholder='Select a model'),\n                         style={'width': '20%'}),\n                dbc.Button(\"Sidebar\", outline=True, color=\"secondary\", className=\"mr-1\", id=\"btn_sidebar\"),\n                dbc.Collapse(\n                    generate_search_bar(),\n                    id=\"navbar-collapse\",\n                    is_open=False,\n                    navbar=True,\n                ),\n            ]\n        ),\n        color=\"dark\",\n        dark=True,\n    )\n    return navbar\n\n\ndef generate_sidebar(SIDEBAR_STYLE):\n    sidebar = html.Div(\n        [\n            html.H2(\"CREMOSYS\", className=\"display-4\"),\n            html.Hr(),\n            html.P(\n                \"CREdit Risk Model MOnitoring SYStem is for a live observation of model's quality\", className=\"lead\"\n            ),\n            dbc.Nav(\n                [\n                    dbc.NavLink(\"Summary\", href=\"/summary\", id=\"summary-link\"),\n                    dbc.NavLink(\"Assess accuracy\", href=\"/page-1\", id=\"page-1-link\"),\n                    dbc.NavLink(\"Assess stability\", href=\"/page-2\", id=\"page-2-link\"),\n                    dbc.NavLink(\"Statistical tests report\", href=\"/page-3\", id=\"page-3-link\"),\n                    dbc.NavLink(\"Generate report\", href=\"/report-generator\", id=\"report-generator-link\"),\n                ],\n                vertical=True,\n                pills=True,\n            ),\n        ],\n        id=\"sidebar\",\n        style=SIDEBAR_STYLE,\n    )\n    return sidebar\ndef generate_footer(FOOTER_STYLE):\n    footer = html.Footer(\n        id='footer',\n        children=[\n            html.H6(\n                \"Copyright \" + u\"\\u00A9\" + \" 2023 Halyk Bank. All rights reserved. Developed by Janysbek Kusmangaliyev\")\n        ],\n        style=FOOTER_STYLE\n    )\n    return footer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# credit_py_validation.py\nfrom scipy.stats import binom\nfrom scipy.stats import norm\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.stats import beta\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn import metrics\nfrom scipy.stats import t\nfrom scipy import stats\n\n\n# def plot_ks(y, y_pred, title=None, xlabel=None, ylabel=None):\n#     if y is None or y_pred is None:\n#         return go.Figure().add_annotation(x=2, y=2, text=\"No Data to Display\",\n#                                           font=dict(family=\"sans serif\", size=25, color=\"crimson\"),\n#                                           showarrow=False, yshift=10)\n#     y = pd.Series(y)\n#     y_pred = pd.Series(y_pred)\n#     n_samples = y.shape[0]\n#     # n_samples = len(y)\n#     n_event = np.sum(y)\n#     n_nonevent = n_samples - n_event\n#\n#     idx = y_pred.argsort()\n#     yy = y[idx]\n#     pp = y_pred[idx]\n#\n#     cum_event = np.cumsum(yy)\n#     cum_population = np.arange(0, n_samples)\n#     cum_nonevent = cum_population - cum_event\n#\n#     p_event = cum_event / n_event\n#     p_nonevent = cum_nonevent / n_nonevent\n#\n#     p_diff = p_nonevent - p_event\n#     print(np.max(abs(p_diff)))\n#     ks_score = np.max(p_diff)\n#     ks_max_idx = np.argmax(p_diff)\n#     # Define the plot settings\n#     print('plot ks: ', ks_score)\n#     if title is None:\n#         title = \"Kolmogorov-Smirnov\"\n#     if xlabel is None:\n#         xlabel = \"Threshold\"\n#     if ylabel is None:\n#         ylabel = \"Cumulative probability\"\n#\n#     # plt.title(title, fontdict={'fontsize': 14})\n#     # plt.xlabel(xlabel, fontdict={'fontsize': 12})\n#     # plt.ylabel(ylabel, fontdict={'fontsize': 12})\n#\n#     fig = go.Figure()\n#     fig.add_trace(go.Scatter(x=pp, y=p_event))\n#     fig.add_trace(go.Scatter(x=pp, y=p_nonevent))\n#     return fig\n\n\ndef kolmogorov_smirnov(default_actual, default_predicted):\n    \"\"\"\n\n     Parameters\n     ----------\n     p : estimated default probability\n     d : number of defaults\n     n : number of obligors\n\n     Returns\n     -------\n     p_value : Binomial test p-value\n\n     Notes\n     -----\n     If the defaults are modeled as iid Bernoulli trials with success probability p,\n     then the number of defaults d is a draw from Binomial(n, p).\n     The one-sided p-value is the probability that such a draw\n     would be at least as large as d.\n     \"\"\"\n\n\n    n_samples = default_actual.shape[0]\n    n_event = np.sum(default_actual)\n    n_nonevent = n_samples - n_event\n\n    idx = default_predicted.argsort()\n    yy = default_actual[idx]\n    pp = default_predicted[idx]\n\n    cum_event = np.cumsum(yy)\n    cum_population = np.arange(0, n_samples)\n    cum_nonevent = cum_population - cum_event\n\n    p_event = cum_event / n_event\n    p_nonevent = cum_nonevent / n_nonevent\n\n    p_diff = p_nonevent - p_event\n\n    ks_score = np.max(p_diff)\n    print('cpv')\n    print(p_diff)\n    ks_max_idx = np.argmax(p_diff)\n    return ks_score\n\n\n\n\n\n\ndef _binomial(p, d, n):\n    \"\"\"\n\n    Parameters\n    ----------\n    p : estimated default probability\n    d : number of defaults\n    n : number of obligors\n\n    Returns\n    -------\n    p_value : Binomial test p-value\n\n    Notes\n    -----\n    If the defaults are modeled as iid Bernoulli trials with success probability p,\n    then the number of defaults d is a draw from Binomial(n, p).\n    The one-sided p-value is the probability that such a draw\n    would be at least as large as d.\n    \"\"\"\n\n    p_value = 1 - binom.cdf(d - 1, n, p)\n\n    return p_value\n\n\ndef binomial_test(data, ratings, default_flag, predicted_pd, alpha_level=0.05):\n    \"\"\"Calculate the Binomial test for a given probability of defaults buckets\n\n    Parameters\n    ----------\n    data : Pandas DataFrame with at least three columns\n            ratings : PD rating class of obligor\n            default_flag : 1 (or True) for defaulted and 0 (or False) for good obligors\n            probs_default : predicted probability of default of an obligor\n\n    ratings : column label for ratings\n    default_flag : column label for default_flag\n    probs_default : column label for probs_default\n    alpha_level : false positive rate of hypothesis test (default .05)\n\n    Returns\n    -------\n    Pandas DataFrame with the following columns :\n        Rating (Index) : Contains the ratings of each class/group\n        PD : predicted default rate in each group\n        N : number of obligors in each group\n        D : number of defaults in each group\n        Default Rate : realised default rate per each group\n        p_value : Binomial test p-value\n        reject : whether to reject the null hypothesis at alpha_level\n\n\n    Notes\n    -----\n    The Binomial test compares forecasted defaults with observed defaults in a binomial\n    model with independent observations under the null hypothesis that the PD applied\n    in the portfolio/rating grade at the beginning of the relevant observation period is\n    greater than the true one (one-sided hypothesis test). The test statistic is the\n    observed number of defaults.\n\n    .. [1] \"Studies on the Validation of Internal Rating Systems,\"\n            Basel Committee on Banking Supervision,\n            p. 47, revised May 2005.\n\n\n    Examples\n    --------\n\n    >>> import random, numpy as np\n    >>> buckets = ['A', 'B', 'C']\n    >>> ratings = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> bucket_pds = {'A': .1, 'B': .15, 'C': .2}\n    >>> probs_default = [bucket_pds[r] for r in ratings]\n    >>> default_flag = [random.uniform(0, 1) < bucket_pds[r] for r in ratings]\n    >>> test_data = pd.DataFrame({'ratings': ratings,\n                                  'default_flag': default_flag,\n                                  'predicted_pd' : probs_default})\n    >>> from meliora import binomial_test\n    >>> binomial_test(test_data, 'ratings', 'default_flag', 'probs_default')\n\n               PD    N   D  Default Rate   p_value  reject\n    ratings\n    A        0.10  401  36      0.089776  0.775347   False\n    B        0.15  489  73      0.149284  0.537039   False\n    C        0.20  110  23      0.209091  0.443273   False\n\n    \"\"\"\n\n    # Perform plausibility checks\n    assert all(x in data.columns for x in [ratings, default_flag, predicted_pd]), \"Missing columns\"\n    assert all(x in [0, False, 1, True] for x in data[default_flag]), \"Default flag can have only value 0 and 1\"\n    assert len(data[ratings].unique()) < 40, \"Number of PD ratings is excessive\"\n    assert all(0 <= x <= 1 for x in data[predicted_pd]), \"Predicted PDs must be between 0% and 100%\"\n\n    # Transform input data into the required format\n    df = data.groupby(ratings).agg({predicted_pd: \"mean\", default_flag: [\"count\", \"sum\", \"mean\"]}).reset_index()\n    df.columns = [\"Rating class\", \"Predicted PD\", \"Total count\", \"Defaults\", \"Actual Default Rate\"]\n\n    # Calculate Binomial test outcome for each rating\n    df[\"p_value\"] = _binomial(df[\"Predicted PD\"], df[\"Defaults\"], df[\"Total count\"])\n    df[\"Reject H0\"] = df[\"p_value\"] < alpha_level\n\n    return df\n\n\ndef _brier(predicted_values, realised_values):\n    \"\"\"\n\n    Parameters\n    ----------\n    predicted_values : Pandas Series of predicted PD outcomes\n    realised_values : Pandas Series of realised PD outcomes\n\n    Returns\n    -------\n    mse : Brier score for the dataset\n\n    Notes\n    -----\n    Calculates the mean squared error (MSE) between the outcomes\n    and their hypothesized PDs. In this context, the MSE is\n    also called the \"Brier score\" of the dataset.\n    \"\"\"\n\n    # Calculate mean squared error\n    errors = realised_values - predicted_values\n    mse = (errors**2).sum()\n\n    return mse\n\n\ndef brier_score(data, ratings, default_flag, predicted_pd):\n    \"\"\"Calculate the Brier score for a given probability of defaults buckets\n\n    Parameters\n    ----------\n    data : Pandas DataFrame with at least three columns\n            ratings : PD rating class of obligor\n            default_flag : 1 (or True) for defaulted and 0 (or False) for good obligors\n            probs_default : predicted probability of default of an obligor\n\n    ratings : column label for ratings\n    default_flag : column label for default_flag\n    probs_default : column label for probs_default\n\n    Returns\n    -------\n    Pandas DataFrame with the following columns :\n        Rating (Index) : Contains the ratings of each class/group\n        PD : predicted default rate in each group and total\n        N : number of obligors in each group and total\n        D : number of defaults in each group and total\n        Default Rate : realised default rate per each group and total\n        brier_score : overall Brier score\n\n\n    Notes\n    -----\n    The Brier score is the mean squared error when each default outcome\n    is predicted by its PD rating. Larger values of the Brier score\n    indicate a poorer performance of the rating system.\n\n    .. [1] \"Studies on the Validation of Internal Rating Systems,\"\n            Basel Committee on Banking Supervision,\n            pp. 46-47, revised May 2005.\n\n\n    Examples\n    --------\n\n    >>> import random, numpy as np\n    >>> buckets = ['A', 'B', 'C']\n    >>> ratings = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> bucket_pds = {'A': .1, 'B': .15, 'C': .2}\n    >>> probs_default = [bucket_pds[r] for r in ratings]\n    >>> default_flag = [random.uniform(0, 1) < bucket_pds[r] for r in ratings]\n    >>> test_data = pd.DataFrame({'ratings': ratings,\n                                  'default_flag': default_flag,\n                                  'predicted_pd' : probs_default})\n    >>> from meliora import brier_score\n    >>> brier_score(test_data, 'ratings', 'default_flag', 'probs_default')\n\n                  PD       N      D  Default Rate brier_score\n    ratings\n    A        0.10000   401.0   36.0      0.089776        None\n    B        0.15000   489.0   73.0      0.149284        None\n    C        0.20000   110.0   23.0      0.209091        None\n    total    0.13545  1000.0  132.0      0.132000    0.113112\n\n    \"\"\"\n\n    # Perform plausibility checks\n    assert all(x in data.columns for x in [ratings, default_flag, predicted_pd]), \"Not all columns are present\"\n    assert all(x in [0, False, 1, True] for x in data[default_flag]), \"Default flag can have only value 0 and 1\"\n    assert len(data[ratings].unique()) < 40, \"Number of PD ratings is excessive\"\n    assert all(x >= 0 and x <= 1 for x in data[predicted_pd]), \"Predicted PDs must be between 0% and 100%\"\n\n    # Transform input data into the required format\n    df = data.groupby(ratings).agg({predicted_pd: \"mean\", default_flag: [\"count\", \"sum\", \"mean\"]})\n    df.columns = [\"PD\", \"N\", \"D\", \"Default Rate\"]\n\n    # Calculate Brier score for the dataset\n    b_score = _brier(df[\"PD\"], df[\"Default Rate\"])\n\n    return b_score\n\n\ndef _herfindahl(df):\n    \"\"\"\n\n    Parameters\n    ----------\n    df : Pandas DataFrame with first column providing the number\n         of obligors and row index corresponding to rating labels\n\n    Returns\n    -------\n    cv : coefficient of variation\n    h : Herfindahl index\n\n    Notes\n    -----\n    Calculates the coefficient of variation and the Herfindahl index,\n    as defined in the paper [1] referenced in herfindahl_test's docstring.\n    These quantities measure the dispersion of rating grades in the data.\n    \"\"\"\n\n    k = df.shape[0]\n    counts = df.iloc[:, 0]\n    n_tot = counts.sum()\n    terms = (counts / n_tot - 1 / k) ** 2\n    cv = (k * terms.sum()) ** 0.5\n    h = (counts**2).sum() / n_tot**2\n\n    return cv, h\n\n\ndef herfindahl_multiple_period_test(data1, data2, ratings, alpha_level=0.05):\n    \"\"\"Calculate the Herfindahl test for a given probability of defaults buckets\n\n    Parameters\n    ----------\n    data1 : Pandas DataFrame with at least one column\n            ratings : PD rating class of obligor\n    data2 : Pandas DataFrame with at least one column\n            ratings : PD rating class of obligor\n\n    ratings : column label for ratings\n    alpha_level : false positive rate of hypothesis test (default .05)\n\n    Returns\n    -------\n    Pandas DataFrame with the following columns :\n        Rating (Index) : Contains the ratings of each class/group\n        N_initial : number of obligors in each group and total\n        h_initial : Herfindahl index for initial dataset\n        N_current : number of obligors in each group and total\n        h_current : Herfindahl index for current dataset\n        p_value : overall Herfindahl test p-value\n        reject : whether to reject the null hypothesis at alpha_level\n\n\n    Notes\n    -----\n    The Herfindahl test looks for an increase in the\n    dispersion of the rating grades over time.\n    The (one-sided) null hypothesis is that the current Herfindahl\n    index is no greater than the initial Herfindahl index.\n    The test statistic is a suitably standardized difference\n    in the coefficient of variation, which is monotonically\n    related to the Herfindahl index.\n    If the Herfindahl index has not changed, then the\n    test statistic has the standard Normal distribution.\n    Large values of this test statistic\n    provide evidence against the null hypothesis.\n    (Note that the reference [1] has an uncommon defintion\n    of Herfindahl index, whereas we return the common definition)\n\n    .. [1] \"Instructions for reporting the validation results\n            of internal models - IRB Pillar I models for credit risks,\" ECB,\n            pp. 26-27, 2019.\n\n\n    Examples\n    --------\n\n    >>> import random, numpy as np\n    >>> buckets = ['A', 'B', 'C']\n    >>> ratings1 = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> test_data1 = pd.DataFrame({'ratings': ratings1})\n    >>> ratings2 = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> test_data2 = pd.DataFrame({'ratings': ratings2})\n    >>> from meliora import herfindahl_test\n    >>> herfindahl_test(test_data1, test_data2, \"ratings\")\n\n           N_initial h_initial  N_current h_current   p_value reject\n    B            489      None        487      None      None   None\n    A            401      None        414      None      None   None\n    C            110      None         99      None      None   None\n    total       1000   0.19291       1000  0.206819  0.475327  False\n\n    \"\"\"\n\n    # Perform plausibility checks\n    assert ratings in data1.columns and ratings in data2.columns, f\"Ratings column {ratings} not found\"\n    assert max(len(data1[ratings].unique()), len(data2[ratings].unique())) < 40, \"Number of PD ratings is excessive\"\n\n    # Transform input data into the required format\n    df1 = pd.DataFrame({\"N_initial\": data1[ratings].value_counts()})\n    df2 = pd.DataFrame({\"N_current\": data2[ratings].value_counts()})\n\n    # Calculate the Herfindahl index for each dataset\n    c1, h1 = _herfindahl(df1)\n    c2, h2 = _herfindahl(df2)\n\n    # Add a row of totals along with Herfindahl indices\n    df1.loc[\"total\"] = [df1[\"N_initial\"].sum()]\n    df1[\"h_initial\"] = None\n    df1.loc[\"total\", \"h_initial\"] = h1\n    df2.loc[\"total\"] = [df2[\"N_current\"].sum()]\n    df2[\"h_current\"] = None\n    df2.loc[\"total\", \"h_current\"] = h2\n\n    # Put the results together into a single dataframe\n    df = df1.join(df2)\n\n    # Calculate Herfindahl test's p-value for the dataset\n    k = df.shape[0] - 1\n    z_stat = (k - 1) ** 0.5 * (c2 - c1) / (c2**2 * (0.5 + c2**2)) ** 0.5\n    p_value = 1 - norm.cdf(z_stat)\n\n    # Put the p-value and test result into the output\n    df[\"p_value\"] = None\n    df.loc[\"total\", \"p_value\"] = p_value\n    if alpha_level:\n        df[\"reject\"] = None\n        df.loc[\"total\", \"reject\"] = p_value < alpha_level\n\n    return df\n\n\ndef herfindahl_test(data1, ratings, alpha_level=0.05):\n    \"\"\"Calculate the Herfindahl test for a given probability of defaults buckets\n\n    Parameters\n    ----------\n    data1 : Pandas DataFrame with at least one column\n            ratings : PD rating class of obligor\n    data2 : Pandas DataFrame with at least one column\n            ratings : PD rating class of obligor\n\n    ratings : column label for ratings\n    alpha_level : false positive rate of hypothesis test (default .05)\n\n    Returns\n    -------\n    Pandas DataFrame with the following columns :\n        Rating (Index) : Contains the ratings of each class/group\n        N_initial : number of obligors in each group and total\n        h_initial : Herfindahl index for initial dataset\n        N_current : number of obligors in each group and total\n        h_current : Herfindahl index for current dataset\n        p_value : overall Herfindahl test p-value\n        reject : whether to reject the null hypothesis at alpha_level\n\n\n    Notes\n    -----\n    The Herfindahl test looks for an increase in the\n    dispersion of the rating grades over time.\n    The (one-sided) null hypothesis is that the current Herfindahl\n    index is no greater than the initial Herfindahl index.\n    The test statistic is a suitably standardized difference\n    in the coefficient of variation, which is monotonically\n    related to the Herfindahl index.\n    If the Herfindahl index has not changed, then the\n    test statistic has the standard Normal distribution.\n    Large values of this test statistic\n    provide evidence against the null hypothesis.\n    (Note that the reference [1] has an uncommon defintion\n    of Herfindahl index, whereas we return the common definition)\n\n    .. [1] \"Instructions for reporting the validation results\n            of internal models - IRB Pillar I models for credit risks,\" ECB,\n            pp. 26-27, 2019.\n\n\n    Examples\n    --------\n\n    >>> import random, numpy as np\n    >>> buckets = ['A', 'B', 'C']\n    >>> ratings1 = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> test_data1 = pd.DataFrame({'ratings': ratings1})\n    >>> ratings2 = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> test_data2 = pd.DataFrame({'ratings': ratings2})\n    >>> from meliora import herfindahl_test\n    >>> herfindahl_test(test_data1, test_data2, \"ratings\")\n\n           N_initial h_initial  N_current h_current   p_value reject\n    B            489      None        487      None      None   None\n    A            401      None        414      None      None   None\n    C            110      None         99      None      None   None\n    total       1000   0.19291       1000  0.206819  0.475327  False\n\n    \"\"\"\n\n    # Perform plausibility checks\n    assert ratings in data1.columns, f\"Ratings column {ratings} not found\"\n    assert len(data1[ratings].unique()) < 40, \"Number of PD ratings is excessive\"\n\n    # Transform input data into the required format\n    df1 = pd.DataFrame({\"N_initial\": data1[ratings].value_counts()})\n\n    # Calculate the Herfindahl index for each dataset\n    c1, h1 = _herfindahl(df1)\n\n    return {'c1': c1, 'h1':h1}\n\n\ndef _hosmer(p, d, n):\n    \"\"\"\n\n    Parameters\n    ----------\n    p : Pandas Series of estimated default probabilities\n    d : Pandas Series of number of defaults\n    n : Pandas Series of number of obligors\n\n    Returns\n    -------\n    p_value : Hosmer-Lemeshow Chi-squared test p-value\n\n    Notes\n    -----\n    Calculates the Hosmer-Lemeshow test statistic, as defined in\n    the paper [1] referenced in hosmer_test's docstring.\n    If the hypothesized PDs are accurate and defaults are independent,\n    this test statisitc is approximately Chi-squared distributed\n    with degrees of freedom equal to the number of rating groups minus two.\n    The p-value is the probability of such a draw being\n    at least as large as the observed value of the statistic.\n    \"\"\"\n\n    assert len(p) > 2, \"Hosmer-Lemeshow test requires at least three groups\"\n\n    # expected_def = n * p\n    # expected_nodef = n * (1 - p)\n    # if any(expected_def < 10) or any(expected_nodef < 10):\n    #     print(\"Warning: a group has fewer than 10 expected defaults or non-defaults.\")\n    #     print(\"--> Chi-squared approximation is questionable.\")\n\n    # terms = (expected_def - d) ** 2 / (p * expected_nodef)\n    # chisq_stat = terms.sum()\n    # p_value = 1 - chi2.cdf(chisq_stat, len(p) - 2)\n\n    kr = sum((d - p * n) ** 2 / (n * p * (1 - p)))  # todo: treatment of missing values\n    p_value = 1 - chi2.cdf(kr, len(p))  # todo: p.val <- pchisq(q = hl, df = k, lower.tail = FALSE)\n\n    return p_value\n\n\ndef hosmer_test(data, ratings, default_flag, predicted_pd, alpha_level=0.05):\n    \"\"\"Calculate the Hosmer-Lemeshow Chi-squared test for a given probability of defaults buckets\n\n    Parameters\n    ----------\n    data : Pandas DataFrame with at least three columns\n            ratings : PD rating class of obligor\n            default_flag : 1 (or True) for defaulted and 0 (or False) for good obligors\n            probs_default : predicted probability of default of an obligor\n\n    ratings : column label for ratings\n    default_flag : column label for default_flag\n    probs_default : column label for probs_default\n    alpha_level : false positive rate of hypothesis test (default .05)\n\n    Returns\n    -------\n    Pandas DataFrame with the following columns :\n        Rating (Index) : Contains the ratings of each class/group\n        PD : predicted default rate in each group and total\n        N : number of obligors in each group and total\n        D : number of defaults in each group and total\n        Default Rate : realised default rate per each group and total\n        p_value : overall Hosmer-Lemeshow test p-value\n        reject : whether to reject the null hypothesis at alpha_level\n\n\n    Notes\n    -----\n    The Hosmer-Lemeshow Chi-squared test calculates a standardized sum\n    of squared differences between the number of defaults and\n    the expected number of defaults within each rating group.\n    Under the null hypothesis that the PDs applied\n    in the portfolio/rating grade at the beginning of the relevant observation period are\n    equal to the true ones, the test statistic has an approximate Chi-squared distribution.\n    Large values of this test statistic\n    provide evidence against the null hypothesis.\n\n    .. [1] \"Backtesting Framework for PD, EAD and LGD - Public Version,\"\n            Bauke Maarse, Rabobank International,\n            p. 43, 2012.\n\n\n    Examples\n    --------\n\n    >>> import random, numpy as np\n    >>> buckets = ['A', 'B', 'C']\n    >>> ratings = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> bucket_pds = {'A': .1, 'B': .15, 'C': .2}\n    >>> probs_default = [bucket_pds[r] for r in ratings]\n    >>> default_flag = [random.uniform(0, 1) < bucket_pds[r] for r in ratings]\n    >>> test_data = pd.DataFrame({'ratings': ratings,\n                                  'default_flag': default_flag,\n                                  'predicted_pd' : probs_default})\n    >>> from meliora import hosmer_test\n    >>> hosmer_test(test_data, 'ratings', 'default_flag', 'probs_default')\n\n                  PD       N      D  Default Rate   p_value reject\n    ratings\n    A        0.10000   401.0   36.0      0.089776      None   None\n    B        0.15000   489.0   73.0      0.149284      None   None\n    C        0.20000   110.0   23.0      0.209091      None   None\n    total    0.13545  1000.0  132.0      0.132000  0.468902  False\n\n    \"\"\"\n\n    # Perform plausibility checks\n    assert all(x in data.columns for x in [ratings, default_flag, predicted_pd]), \"Not all columns are present\"\n    assert all(x in [0, False, 1, True] for x in data[default_flag]), \"Default flag can have only value 0 and 1\"\n    assert len(data[ratings].unique()) < 40, \"Number of PD ratings is excessive\"\n    assert all(x >= 0 and x <= 1 for x in data[predicted_pd]), \"Predicted PDs must be between 0% and 100%\"\n\n    # Transform input data into the required format\n    df = data.groupby(ratings).agg({predicted_pd: \"mean\", default_flag: [\"count\", \"sum\", \"mean\"]})\n    df.columns = [\"PD\", \"N\", \"D\", \"Default Rate\"]\n\n    # Calculate Hosmer-Lemeshow test's p-value for the dataset\n    p_value = _hosmer(df[\"PD\"], df[\"D\"], df[\"N\"])\n\n    return {'p_value': p_value, 'p_val < a': bool(p_value < alpha_level)}\n\n\ndef _spiegelhalter(realised_values, predicted_values, alpha_level=0.05):\n    \"\"\"\n    todo: https://github.com/andrija-djurovic/PDtoolkit/blob/main/R/12_PREDICTIVE_POWER.R\n\n    Parameters\n    ----------\n    ratings : Pandas Series of rating categories\n    default_flag : Pandas Series of default outcomes (0/1 or False/True)\n    df : Pandas DataFrame with ratings as rownames and a column of hypothesized 'PD' values\n\n    Returns\n    -------\n    p_value : Spiegelhalter test p-value\n\n    Notes\n    -----\n    Calculates the mean squared error (MSE) between the outcomes\n    and their hypothesized PDs, which is approximately Normal.\n    If the hypothesized PDs equal the true PDs, then the mean\n    and standard deviation of that statistic are provided in\n    the paper [1] referenced in spiegelhalter_test's docstring.\n    The standardized statistic is approximately standard Normal.\n    and leads to a \"one-sided\" p-value via the Normal cdf.\n    \"\"\"\n\n    # Calculate mean squared error\n    errors = realised_values - predicted_values\n    mse = (errors**2).sum() / len(errors)\n\n    # # Calculate null expectation and variance of MSE\n    expectations = sum(predicted_values * (1 - predicted_values)) / len(realised_values)\n    variances = (\n        sum(predicted_values * (1 - 2 * predicted_values) ** 2 * (1 - predicted_values)) / len(realised_values) ** 2\n    )\n\n    # Calculate standardized statistic\n    z_score = (mse - expectations) / np.sqrt(variances)  # todo: check formula\n\n    # Calculate standardized MSE as test statistic, then its p-value\n    outcome = z_score > norm.ppf(1 - alpha_level / 2)\n\n    return {'z_score': z_score, 'outcome':bool(outcome)}\n\n\ndef spiegelhalter_test(data, ratings, default_flag, predicted_pd, alpha_level=0.05):\n    \"\"\"Calculate the Spiegelhalter test for a given probability of defaults buckets\n\n    Parameters\n    ----------\n    data : Pandas DataFrame with at least three columns\n            ratings : PD rating class of obligor\n            default_flag : 1 (or True) for defaulted and 0 (or False) for good obligors\n            probs_default : predicted probability of default of an obligor\n\n    ratings : column label for ratings\n    default_flag : column label for default_flag\n    probs_default : column label for probs_default\n    alpha_level : false positive rate of hypothesis test (default .05)\n\n    Returns\n    -------\n    Pandas DataFrame with the following columns :\n        Rating (Index) : Contains the ratings of each class/group\n        PD : predicted default rate in each group and total\n        N : number of obligors in each group and total\n        D : number of defaults in each group and total\n        Default Rate : realised default rate per each group and total\n        p_value : overall Spiegelhalter test p-value\n        reject : whether to reject the null hypothesis at alpha_level\n\n\n    Notes\n    -----\n    The Spiegelhalter test compares forecasted defaults with observed defaults by analyzing\n    the prediction errors. Under the null hypothesis that the PDs applied\n    in the portfolio/rating grade at the beginning of the relevant observation period are\n    equal to the true ones, the mean squared error can be standardized into\n    an approximately standard Normal test statistic. Large values of this test statistic\n    provide evidence against the null hypothesis.\n\n    .. [1] \"Backtesting Framework for PD, EAD and LGD - Public Version,\"\n            Bauke Maarse, Rabobank International,\n            pp. 43-44, 2012.\n\n\n    Examples\n    --------\n\n    >>> import random, numpy as np\n    >>> buckets = ['A', 'B', 'C']\n    >>> ratings = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> bucket_pds = {'A': .1, 'B': .15, 'C': .2}\n    >>> probs_default = [bucket_pds[r] for r in ratings]\n    >>> default_flag = [random.uniform(0, 1) < bucket_pds[r] for r in ratings]\n    >>> test_data = pd.DataFrame({'ratings': ratings,\n                                  'default_flag': default_flag,\n                                  'predicted_pd' : probs_default})\n    >>> from meliora import spiegelhalter_test\n    >>> spiegelhalter_test(test_data, 'ratings', 'default_flag', 'probs_default')\n\n                  PD       N      D  Default Rate   p_value reject\n    ratings\n    A        0.10000   401.0   36.0      0.089776      None   None\n    B        0.15000   489.0   73.0      0.149284      None   None\n    C        0.20000   110.0   23.0      0.209091      None   None\n    total    0.13545  1000.0  132.0      0.132000  0.647161  False\n\n    \"\"\"\n\n    # Perform plausibility checks\n    assert all(x in data.columns for x in [ratings, default_flag, predicted_pd]), \"Not all columns are present\"\n    assert all(x in [0, False, 1, True] for x in data[default_flag]), \"Default flag can have only value 0 and 1\"\n    assert len(data[ratings].unique()) < 40, \"Number of PD ratings is excessive\"\n    assert all(x >= 0 and x <= 1 for x in data[predicted_pd]), \"Predicted PDs must be between 0% and 100%\"\n\n    # Transform input data into the required format\n    df = data.groupby(ratings).agg({predicted_pd: \"mean\", default_flag: [\"count\", \"sum\", \"mean\"]})\n    df.columns = [\"PD\", \"N\", \"D\", \"Default Rate\"]\n\n    # Calculate Spiegelhalter test's p-value for the dataset\n    result = _spiegelhalter(df[\"PD\"], df[\"Default Rate\"])\n\n    return result\n\n\ndef _jeffreys(p, d, n):\n    \"\"\"\n\n    Parameters\n    ----------\n    p : estimated default probability\n    d : number of defaults\n    n : number of obligors\n\n    Returns\n    -------\n    p_value : Jeffrey's \"p-value\" (The posterior probability of the null hypothesis)\n\n    Notes\n    -----\n    Given the Jeffreys prior for the binomial proportion, the\n    posterior distribution is a beta distribution with shape parameters a = D + 1/2 and\n    b = N − D + 1/2. Here, N is the number of customers in the portfolio/rating grade and\n    D is the number of those customers that have defaulted within that observation\n    period. The p-value (i.e. the cumulative distribution function of the aforementioned\n    beta distribution evaluated at the PD of the portfolio/rating grade) serves as a\n    measure of the adequacy of estimated PD.\n    \"\"\"\n\n    a = d + 0.5\n    b = n - d + 0.5\n    p_value = beta.cdf(p, a, b)\n\n    return p_value\n\n\ndef jeffreys_test(data, ratings, default_flag, predicted_pd, alpha_level=0.05):\n    \"\"\"Calculate the Jeffrey's test for a given probability of defaults buckets\n\n    Parameters\n    ----------\n    data : Pandas DataFrame with at least three columns\n            ratings : PD rating class of obligor\n            default_flag : 1 (or True) for defaulted and 0 (or False) for good obligors\n            probs_default : predicted probability of default of an obligor\n\n    ratings : column label for ratings\n    default_flag : column label for default_flag\n    probs_default : column label for probs_default\n    alpha_level : false positive rate of hypothesis test (default .05)\n\n    Returns\n    -------\n    Pandas DataFrame with the following columns :\n        Rating (Index) : Contains the ratings of each class/group\n        PD : predicted default rate in each group\n        N : number of obligors in each group\n        D : number of defaults in each group\n        Default Rate : realised default rate per each group\n        p_value : Jeffreys p-value\n        reject : whether to reject the null hypothesis at alpha_level\n\n\n    Notes\n    -----\n    The Jeffreys test compares forecasted defaults with observed defaults in a binomial\n    model with independent observations under the null hypothesis that the PD applied\n    in the portfolio/rating grade at the beginning of the relevant observation period is\n    greater than the true one (one-sided hypothesis test). The test updates a Beta distribution\n    (with Jeffrey's prior) in light of the number of defaults and non-defaults,\n    then reports the posterior probability of the null hypothesis.\n\n    .. [1] \"Instructions for reporting the validation results\n            of internal models - IRB Pillar I models for credit risks,\" ECB,\n            pp. 20-21, 2019.\n\n\n    Examples\n    --------\n\n    >>> import random, numpy as np\n    >>> buckets = ['A', 'B', 'C']\n    >>> ratings = random.choices(buckets,  [0.4, 0.5, 0.1], k=1000)\n    >>> bucket_pds = {'A': .1, 'B': .15, 'C': .2}\n    >>> probs_default = [bucket_pds[r] for r in ratings]\n    >>> default_flag = [random.uniform(0, 1) < bucket_pds[r] for r in ratings]\n    >>> test_data = pd.DataFrame({'ratings': ratings,\n                                  'default_flag': default_flag,\n                                  'predicted_pd' : probs_default})\n    >>> from meliora import jeffreys_test\n    >>> jeffreys_test(test_data, 'ratings', 'default_flag', 'probs_default')\n\n               PD    N   D  Default Rate   p_value  reject\n    ratings\n    A        0.10  401  36      0.089776  0.748739   False\n    B        0.15  489  73      0.149284  0.511781   False\n    C        0.20  110  23      0.209091  0.397158   False\n\n    \"\"\"\n\n    # Perform plausibility checks\n    assert all(x in data.columns for x in [ratings, default_flag, predicted_pd]), \"Not all columns are present\"\n    assert all(x in [0, False, 1, True] for x in data[default_flag]), \"Default flag can have only value 0 and 1\"\n    assert len(data[ratings].unique()) < 40, \"Number of PD ratings is excessive\"\n    assert all(x >= 0 and x <= 1 for x in data[predicted_pd]), \"Predicted PDs must be between 0% and 100%\"\n\n    # Transform input data into the required format\n    df = data.groupby(ratings).agg({predicted_pd: \"mean\", default_flag: [\"count\", \"sum\", \"mean\"]}).reset_index()\n    df.columns = [\"Rating class\", \"Predicted PD\", \"Total count\", \"Defaults\", \"Actual Default Rate\"]\n\n    # Calculate Binomial test outcome for each rating\n    df[\"p_value\"] = _jeffreys(df[\"Predicted PD\"], df[\"Defaults\"], df[\"Total count\"])\n    df[\"Reject H0\"] = df[\"p_value\"] < alpha_level\n\n    return df\n\n\ndef roc_auc(df, target, prediction):\n    \"\"\"Compute Area ROC AUC from prediction scores.\n\n    Note: this implementation can be used with binary, multiclass and\n    multilabel classification, but some restrictions apply (see Parameters).\n    Read more in the :ref:`User Guide <roc_metrics>`.\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n        True labels or binary label indicators. The binary and multiclass cases\n        expect labels with shape (n_samples,) while the multilabel case expects\n        binary label indicators with shape (n_samples, n_classes).\n    y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n        Target scores.\n\n    Returns\n    -------\n    auc : float\n        Area Under the Curve score.\n\n    See Also\n    --------\n    https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/metrics/_ranking.py#L47\n    \"\"\"\n\n    # Perform plausibility checks\n    assert all(x >= 0 and x <= 1 for x in df[target]), \"Predicted PDs must be between 0% and 100%\"\n    assert all(x >= 0 and x <= 1 for x in df[prediction]), \"Predicted PDs must be between 0% and 100%\"\n\n    return roc_auc_score(df[target], df[prediction])\n\n\ndef clar(df, predicted_ratings, realised_outcomes):\n    \"\"\"\n    CLAR serves as a measure of ranking ability against LGD risk\n    The cumulative LGD accuracy ratio (CLAR) curve can be treated as\n    the equivalent of the Cumulative Accuracy Profile (CAP) curve. This\n    test compares the cumulative percentage of correctly assigned realized\n    LGD and the cumulative rate of observations in the predicted LGD bands.\n    Parameters\n    ----------\n    predicted_ratings: pandas Series\n        predicted LGD, can be ordinal or continuous\n    realised_outcomes: pandas Series\n        realised LGD, can be ordinal or continuous\n    Returns\n    -------\n    clar: scalar\n        Cumulative LGD Accuracy Ratio\n    References\n    --------------\n    [1] Ozdemir, B., Miu, P., 2009. Basel II Implementation.\n    A Guide to Developing and Validating a Compliant Internal Risk Rating\n    System. McGraw-Hill, USA.\n    [2] See also: https://rdrr.io/cran/VUROCS/man/clar.html\n    Examples\n    --------\n        >>> res = clar(predicted_ratings, realised_outcomes)\n        >>> print(res)\n    \"\"\"\n\n    # Calculate CLAR\n    x_s = [0]\n    x_values = [0]\n    y_values = [0]\n\n    for i, j in enumerate(list(set(df[predicted_ratings]))[::-1]):\n        x = (df[predicted_ratings] == j).sum()\n        x_bucket = df.sort_values(by=realised_outcomes, ascending=False)[x_s[i]: x_s[i] + x]\n        x_value = x / len(df)\n        y_value = (x_bucket[realised_outcomes] == j).sum() / len((x_bucket[realised_outcomes] == j))\n        x_values.append(x_value)\n        y_values.append(y_value)\n        x_s.append(x + 1)\n\n    new_xvalues = list(np.cumsum(x_values))\n    new_yvalues = list(np.cumsum(y_values))\n\n    model_auc = auc(new_xvalues, new_yvalues)\n    clar = 2 * model_auc\n\n    return clar\n\n\ndef loss_capture_ratio(ead, predicted_ratings, realised_outcomes):\n    \"\"\"\n    The loss_capture_ratio measures how well a model is able to\n    rank LGDs when compared to the observed losses.\n    For this approach three plots are relevant: the model loss\n    capture curve, ideal loss capture curve and the random loss\n    capture curve. These curves are constructed in the same way\n    as the curves for the CAP. The main difference is the data,\n    which is for LGDs and the LR a (continuous) percentage of the EAD,\n    while for the CAP it is binary.\n    The LC can be percentage weighted, which simply uses the LGD and\n    LR percentages as input, while it can also be EAD weighted, which\n    uses the LGD and LR multiplied with the respective EAD as input.\n    The results between the two approaches can differ  if the portfolio\n    is not-well balanced.\n    Parameters\n    ----------\n    ead: pandas Series\n        Exposure at Default\n    predicted_ratings: pandas Series\n        predicted LGD, can be ordinal or continuous\n    realised_outcomes: pandas Series\n        realised LGD, can be ordinal or continuous\n    Returns\n    -------\n    LCR: scalar\n        Loss Capture Ratio\n    References\n    ----------------\n    Li, D., Bhariok, R., Keenan, S., & Santilli, S. (2009). Validation techniques\n    and performance metrics for loss given default models.\n    The Journal of Risk Model Validation, 33, 3-26.\n    Examples\n    --------\n        >>> res = loss_capture_ratio(ead, predicted_ratings, realised_outcomes)\n        >>> print(res)\n    \"\"\"\n\n    # Create a dataframe\n    frame = {\"ead\": ead, \"predicted_ratings\": predicted_ratings, \"realised_outcomes\": realised_outcomes}\n    df = pd.DataFrame(frame)\n\n    # Prepare data\n    df[\"loss\"] = df[\"ead\"] * df[\"realised_outcomes\"]\n\n    # Model loss capture curve\n    df2 = df.sort_values(by=\"predicted_ratings\", ascending=False)\n    df2[\"cumulative_loss\"] = df2.cumsum()[\"loss\"]\n    df2[\"cumulative_loss_capture_percentage\"] = df2.cumsum()[\"loss\"] / df2.loss.sum()\n    auc_curve1 = auc([i for i in range(len(df2))], df2.cumulative_loss_capture_percentage)\n    random_auc1 = 0.5 * len(df2) * 1\n\n    # Ideal loss capture curve\n    df3 = df.sort_values(by=\"realised_outcomes\", ascending=False)\n    df3[\"cumulative_loss\"] = df3.cumsum()[\"loss\"]\n    df3[\"cumulative_loss_capture_percentage\"] = df3.cumsum()[\"loss\"] / df3.loss.sum()\n    auc_curve2 = auc([i for i in range(len(df3))], df3.cumulative_loss_capture_percentage)\n    random_auc2 = 0.5 * len(df3) * 1\n\n    loss_capture_ratio = (auc_curve1 - random_auc1) / (auc_curve2 - random_auc2)\n\n    return loss_capture_ratio\n\n\ndef bayesian_error_rate(default_flag, prob_default):\n    \"\"\"\n    BER is the proportion of the whole sample that is misclassified\n    when the rating system is in optimal use. For a perfect rating model,\n    the BER has a value of zero. A model's BER depends on the probability\n    of default. The lower the BER, and the lower the classification error,\n    the better the model.\n    The Bayesian error rate specifies the minimum probability of error if\n    the rating system or score function under consideration is used for a\n    yes/no decision whether a borrower will default or not. The error can\n    be estimated parametrically, e.g. assuming normal score distributions,\n    or non-parametrically, for instance with kernel density estimation methods.\n    If parametric estimation is applied, the distributional assumptions have\n    to be carefully checked. Non-parametric estimation will be critical if\n    sample sizes are small. In its general form, the error rate depends on\n    the total portfolio probability of default. As a consequence, in many\n    cases its magnitude is influenced much more by the probability of\n    erroneously identifying a non-defaulter as a defaulter than by the\n    probability of not detecting a defaulter.\n    In practice, therefore, the error rate is often applied\n    with a fictitious 50% probability of default. In this case, the error\n    rate is equivalent to the Kolmogorov-Smirnov statistic and to the Pietra index.\n    Parameters\n    ----------\n    default_flag : pandas series\n        Boolean flag indicating whether the borrower has actually defaulted\n    prob_default : pandas series\n        Predicted default probability, as returned by a classifier.\n    Returns\n    ---------\n    score : float\n        Bayesian Error Rate.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> default_flag = [1, 0, 0, 1, 1]\n    >>> prob_default = [0.01, 0.04, 0.07, 0.11, 0]\n    >>> bayesian_error_rate(default_flag, prob_default)\n    -0.47140452079103173\n    \"\"\"\n\n    frame = {\"default_flag\": default_flag, \"prob_default\": prob_default}\n\n    df = pd.DataFrame(frame)\n\n    fpr, tpr, thresholds = metrics.roc_curve(df[\"default_flag\"], df[\"prob_default\"])\n    roc_curve_df = pd.DataFrame({\"c\": thresholds, \"hit_rate\": tpr, \"false_alarm_rate\": fpr})\n\n    p_d = df.default_flag.sum() / len(df)\n\n    roc_curve_df[\"ber\"] = p_d * (1 - roc_curve_df.hit_rate) + (1 - p_d) * roc_curve_df.false_alarm_rate\n\n    return round(min(roc_curve_df[\"ber\"]), 3)\n\n\ndef calc_iv(df, feature, target, pr=0):\n    \"\"\"\n    A numerical value that quantifies the predictive power of an independent\n    variable in capturing the binary dependent variable.\n    Weight of evidence (WOE) is a measure of how much the evidence supports or\n    undermines a hypothesis. WOE measures the relative risk of an attribute of\n    binning level. The value depends on whether the value of the target variable\n    is a nonevent or an event.\n    The information value (IV) is a weighted sum of the WOE of the\n    characteristic's attributes. The weight is the difference between the\n    conditional probability of an attribute for an event and the conditional\n    probability of that attribute for a nonevent.\n    An information value can be any real number. Generally speaking, the higher\n    the information value, the more predictive an attribute is likely to be.\n    Parameters\n    ----------\n    df : Pandas dataframe\n        Contains information on the the feature and target variable\n    feature : string\n        independent variable\n    feature : string\n        dependent variable\n    Returns\n    -------\n    iv : float\n       Information Value.\n    References\n    --------------\n    -  https://www.lexjansen.com/mwsug/2013/AA/MWSUG-2013-AA14.pdf.\n    -  https://documentation.sas.com/doc/en/vdmmlcdc/8.1/casstat/viyastat_binning_details02.htm.\n    Examples\n    --------\n    >>> iv = calc_iv(df, feature, target, pr=0)\n    >>> iv\n    -0.47140452079103173\n\n    \"\"\"\n\n    lst = []\n\n    for i in range(df[feature].nunique()):\n        val = list(df[feature].unique())[i]\n        lst.append(\n            [\n                feature,\n                val,\n                df[df[feature] == val].count()[feature],\n                df[(df[feature] == val) & (df[target] == 1)].count()[feature],\n            ]\n        )\n\n    data = pd.DataFrame(lst, columns=[\"Variable\", \"Value\", \"All\", \"Bad\"])\n    data = data[data[\"Bad\"] > 0]\n\n    data[\"Share\"] = data[\"All\"] / data[\"All\"].sum()\n    data[\"Bad Rate\"] = data[\"Bad\"] / data[\"All\"]\n    data[\"Distribution Good\"] = (data[\"All\"] - data[\"Bad\"]) / (data[\"All\"].sum() - data[\"Bad\"].sum())\n    data[\"Distribution Bad\"] = data[\"Bad\"] / data[\"Bad\"].sum()\n    data[\"WoE\"] = np.log(data[\"Distribution Good\"] / data[\"Distribution Bad\"])\n    data[\"IV\"] = data[\"WoE\"] * (data[\"Distribution Good\"] - data[\"Distribution Bad\"])\n\n    data = data.sort_values(by=[\"Variable\", \"Value\"], ascending=True)\n    iv = data[\"IV\"].sum()\n\n    return data, iv\n\n\ndef lgd_t_test(df, observed_LGD_col, expected_LGD_col, verbose=False):\n    \"\"\"t-test for the Null hypothesis that estimated LGD is greater than true LGD\n    Parameters\n    ----------\n    df: array-like, at least 2D\n        data\n    observed_LGD_col: string\n        name of column with observed LGD values\n    expected_LGD_col: string\n        name of column with expected LGD values\n    verbose: boolean\n        if true, results and interpretation are printed\n    Returns\n    -------\n    N: integer\n        Number of customers\n    LGD.mean: float\n        Mean value of observed LGD values\n    pred_LGD.mean: float\n        Mean value of predicted LGD values\n    t_stat: float\n        test statistic\n    lgd_s2: float\n        denominator of test statistic\n    p_value: float\n        p-value of the test\n    Notes\n    -----------\n    Observations are assumed to be independent.\n    This fundtion can be used for both performing and non-performing LGDs.\n    Examples\n    --------\n    .. code-block:: python\n        >>> res = lgd_t_test(df=df, observed_LGD_col='LGD', expected_LGD_col='PRED_LGD', verbose=True)\n        >>> print(res)\n    \"\"\"\n    # Checking for any missing data\n    if df.empty:\n        raise TypeError(\"No data provided!\")\n    if observed_LGD_col is None:\n        raise TypeError(\"No column name for observed LGDs provided\")\n    if expected_LGD_col is None:\n        raise TypeError(\"No column name for expected LGDs provided\")\n\n    # Checking that the correct datatype\n    if not isinstance(observed_LGD_col, str):\n        raise TypeError(\"observed_LGD_col not of type string\")\n    if not isinstance(expected_LGD_col, str):\n        raise TypeError(\"expected_LGD_col not of type string\")\n\n    # Check if the correct column names have been provided\n    if observed_LGD_col not in df.columns:\n        raise ValueError(\"{} not a column in the df\".format(observed_LGD_col))\n    if expected_LGD_col not in df.columns:\n        raise ValueError(\"{} not a column in the df\".format(expected_LGD_col))\n\n    # Check the data for missing values\n    if df[observed_LGD_col].hasnans:\n        raise ValueError(\"Missing values in {}\".format(observed_LGD_col))\n    if df[expected_LGD_col].hasnans:\n        raise ValueError(\"Missing values in {}\".format(expected_LGD_col))\n\n    N = len(df)\n    LGD = df[observed_LGD_col]\n    pred_LGD = df[expected_LGD_col]\n    error = LGD - pred_LGD\n    mean_error = error.mean()\n    num = np.sqrt(N) * mean_error\n    lgd_s2 = ((error - mean_error) ** 2).sum() / (N - 1)\n    t_stat = num / np.sqrt(lgd_s2)\n    p_value = 1 - t.cdf(t_stat, df=N - 1)\n\n    if verbose is True:\n        # print the results\n        print(\n            \"t_stat=%.3f, LGD.mean=%.3f,pred_LGD.mean=%.3f,N=%d, s2=%.3f, p=%.3f\"\n            % (t_stat, pred_LGD.mean(), LGD.mean(), N, lgd_s2, p_value)\n        )\n        if p_value <= 0.05:\n            print(\"P-value <= 5%, therefore, H0 is rejected.\")\n        elif p_value > 0.05:\n            print(\"P-value > 5%, therefore, H0 fails to be rejected.\")\n\n    return N, LGD.mean(), pred_LGD.mean(), t_stat, lgd_s2, p_value\n\n\ndef migration_matrix_stability(df, initial_ratings_col, final_ratings_col):\n    \"\"\"z-tests to verify stability of transition matrices\n    Parameters\n    ----------\n    df: array-like, at least 2D\n        data\n    initial_ratings_col: string\n        name of column with initial ratings values\n    final_ratings_col: string\n        name of column with final ratings values\n    Returns\n    -------\n    z_df: array-like\n        z statistic for each ratings pair\n    phi_df: array-like\n        p-values for each ratings pair\n    Notes\n    -----------\n    The Null hypothesis is that p_ij >= p_ij-1 or p_ij-1 >= p_ij\n    depending on whether the (ij) entry is below or above main diagonal\n    Examples\n    --------\n    .. code-block:: python\n        >>> res = migration_matrix_stability(df=df, initial_ratings_col='ratings', final_ratings_col='ratings2')\n        >>> print(res)\n    \"\"\"\n    a = df[initial_ratings_col]\n    b = df[final_ratings_col]\n    N_ij = pd.crosstab(a, b)\n    p_ij = pd.crosstab(a, b, normalize=\"index\")\n    K = len(set(a))\n    z_df = p_ij.copy()\n    for i in range(1, K + 1):\n        for j in range(1, K + 1):\n            if i == j:\n\n                z_ij = np.nan\n\n            if i > j:\n                Ni = N_ij.sum(axis=1).values[i - 1]\n\n                num = p_ij.iloc[i - 1, j - 1 + 1] - p_ij.iloc[i - 1, j - 1]\n                den_a = p_ij.iloc[i - 1, j - 1] * (1 - p_ij.iloc[i - 1, j - 1]) / Ni\n                den_b = p_ij.iloc[i - 1, j - 1 + 1] * (1 - p_ij.iloc[i - 1, j - 1 + 1]) / Ni\n                den_c = 2 * p_ij.iloc[i - 1, j - 1] * p_ij.iloc[i - 1, j - 1 + 1] / Ni\n\n                z_ij = num / np.sqrt(den_a + den_b + den_c)\n\n            elif i < j:\n                Ni = N_ij.sum(axis=1).values[i - 1]\n\n                num = p_ij.iloc[i - 1, j - 1 - 1] - p_ij.iloc[i - 1, j - 1]\n                den_a = p_ij.iloc[i - 1, j - 1] * (1 - p_ij.iloc[i - 1, j - 1]) / Ni\n                den_b = p_ij.iloc[i - 1, j - 1 - 1] * (1 - p_ij.iloc[i - 1, j - 1 - 1]) / Ni\n                den_c = 2 * p_ij.iloc[i - 1, j - 1] * p_ij.iloc[i - 1, j - 1 - 1] / Ni\n\n                z_ij = num / np.sqrt(den_a + den_b + den_c)\n\n            else:\n\n                z_ij = np.nan\n\n            z_df.iloc[i - 1, j - 1] = z_ij\n    phi_df = z_df.apply(lambda x: x.apply(lambda y: norm.cdf(y)))\n    return z_df, phi_df\n\n\ndef psi(actual, actual_bins, expected, expected_bins, crosstab=False):  # todo: expected vs actual\n    \"\"\"Calculate the PSI for a single variable\n    Args:\n        expected_array: numpy array of original values\n        actual_array: numpy array of new values, same size as expected\n        buckets: number of percentile ranges to bucket the values into\n    Returns:\n        psi_value: calculated PSI value\n\n    \"\"\"\n\n    if crosstab:\n        actual = pd.crosstab(actual, actual_bins).sum()\n        expected = pd.crosstab(expected, expected_bins).sum()\n    actual = actual/actual.sum()\n    expected = expected/expected.sum()\n    psi = (actual - expected) * np.log(actual/expected)\n    psi = np.sum(psi)\n    # df.columns = [\"actual\", \"expected\"]\n\n    # df[\"expected\"] = np.where(df[\"expected\"] == 0, 0.0001, df[\"expected\"])\n\n    # # Calculating PSI\n    # df[\"PSI\"] = (df[\"actual\"] - df[\"expected\"]) * np.log(df[\"actual\"] / df[\"expected\"])\n\n    # psi = np.sum(df[\"PSI\"])\n\n    return psi\n\n\ndef kendall_tau(x, y, variant=\"b\"):\n    \"\"\"\n    Calculate Kendall's tau, a correlation measure for ordinal data.\n    This is a wrapper around SciPy kendalltau function.\n    Kendall's tau is a measure of the correspondence between two rankings.\n    Values close to 1 indicate strong agreement, and values close to -1\n    indicate strong disagreement. This implements two variants of Kendall's\n    tau: tau-b (the default) and tau-c (also known as Stuart's tau-c). These\n    differ only in how they are normalized to lie within the range -1 to 1;\n    the hypothesis tests (their p-values) are identical. Kendall's original\n    tau-a is not implemented separately because both tau-b and tau-c reduce\n    to tau-a in the absence of ties.\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of rankings, of the same shape. If arrays are not 1-D, they\n        will be flattened to 1-D.\n    variant: {'b', 'c'}, optional\n        Defines which variant of Kendall's tau is returned. Default is 'b'.\n    Returns\n    -------\n    correlation : float\n       The tau statistic.\n    pvalue : float\n       The p-value for a hypothesis test whose null hypothesis is\n       an absence of association, tau = 0.\n    References\n    --------------\n    [1] Maurice G. Kendall, \"A New Measure of Rank Correlation\", Biometrika\n           Vol. 30, No. 1/2, pp. 81-93, 1938.\n    [2] Maurice G. Kendall, \"The treatment of ties in ranking problems\",\n           Biometrika Vol. 33, No. 3, pp. 239-251. 1945.\n    [3] Gottfried E. Noether, \"Elements of Nonparametric Statistics\",\n        John Wiley & Sons, 1967.\n    [4] Peter M. Fenwick, \"A new data structure for cumulative frequency tables\",\n        Software: Practice and Experience, Vol. 24, No. 3, pp. 327-336, 1994.\n    [5] Maurice G. Kendall, \"Rank Correlation Methods\" (4th Edition),\n           Charles Griffin & Co., 1970.\n    Scipy: https://github.com/scipy/scipy/blob/v1.8.1/scipy/stats/_stats_py.py#L4666-L4875\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x1 = [12, 2, 1, 12, 2]\n    >>> x2 = [1, 4, 7, 1, 0]\n    >>> tau, p_value = kendall_tau(x1, x2)\n    >>> tau\n    -0.47140452079103173\n    >>> p_value\n    0.2827454599327748\n    \"\"\"\n\n    tau, pvalue = stats.kendalltau(x, y, initial_lexsort=None, variant=\"b\")\n\n    return tau, pvalue\n\n\ndef somersd(array_1, array_2, alternative=\"two-sided\"):\n    \"\"\"\n    Calculates Somers' D, an asymmetric measure of ordinal association.\n    This is a wrapper around scipy.stats.somersd function.\n    Somers' :math:`D` is a measure of the correspondence between two rankings.\n    It considers the difference between the number of concordant\n    and discordant pairs in two rankings and is  normalized such that values\n    close  to 1 indicate strong agreement and values close to -1 indicate\n    strong disagreement.\n    Parameters\n    ----------\n    x: array_like\n        1D array of rankings, treated as the (row) independent variable.\n        Alternatively, a 2D contingency table.\n    y: array_like, optional\n        If `x` is a 1D array of rankings, `y` is a 1D array of rankings of the\n        same length, treated as the (column) dependent variable.\n        If `x` is 2D, `y` is ignored.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n        * 'two-sided': the rank correlation is nonzero\n        * 'less': the rank correlation is negative (less than zero)\n        * 'greater':  the rank correlation is positive (greater than zero)\n    Returns\n    -------\n    res : SomersDResult\n        A `SomersDResult` object with the following fields:\n            correlation : float\n               The Somers' :math:`D` statistic.\n            pvalue : float\n               The p-value for a hypothesis test whose null\n               hypothesis is an absence of association, :math:`D=0`.\n               See notes for more information.\n            table : 2D array\n               The contingency table formed from rankings `x` and `y` (or the\n               provided contingency table, if `x` is a 2D array)\n    References\n    ----------\n    [1] Robert H. Somers, \"A New Asymmetric Measure of Association for\n           Ordinal Variables\", *American Sociological Review*, Vol. 27, No. 6,\n           pp. 799--811, 1962.\n    [2] Morton B. Brown and Jacqueline K. Benedetti, \"Sampling Behavior of\n           Tests for Correlation in Two-Way Contingency Tables\", *Journal of\n           the American Statistical Association* Vol. 72, No. 358, pp.\n           309--315, 1977.\n    [3] SAS Institute, Inc., \"The FREQ Procedure (Book Excerpt)\",\n           *SAS/STAT 9.2 User's Guide, Second Edition*, SAS Publishing, 2009.\n    [4] Laerd Statistics, \"Somers' d using SPSS Statistics\", *SPSS\n           Statistics Tutorials and Statistical Guides*,\n           https://statistics.laerd.com/spss-tutorials/somers-d-using-spss-statistics.php,\n           Accessed July 31, 2020.\n    Examples\n    --------\n    >>> table = [[27, 25, 14, 7, 0], [7, 14, 18, 35, 12], [1, 3, 2, 7, 17]]\n    >>> res = somersd(table)\n    >>> res.statistic\n    0.6032766111513396\n    >>> res.pvalue\n    1.0007091191074533e-27\n\n    \"\"\"\n\n    return stats.somersd(array_1, array_2, alternative=\"two-sided\")\n\n\ndef spearman_corr(array_1, array_2, alternative=\"two-sided\"):\n    \"\"\"\n    Calculate a Spearman correlation coefficient with associated p-value.\n    This is a wrapper around scipy.stats.spearmanr function.\n    The Spearman rank-order correlation coefficient is a nonparametric\n    measure of the monotonicity of the relationship between two datasets.\n    Unlike the Pearson correlation, the Spearman correlation does not\n    assume that both datasets are normally distributed. Like other\n    correlation coefficients, this one varies between -1 and +1 with 0\n    implying no correlation. Correlations of -1 or +1 imply an exact\n    monotonic relationship. Positive correlations imply that as x\n    increases, so does y. Negative correlations imply that as x increases,\n    y decreases.\n    The p-value roughly indicates the probability of an uncorrelated\n    system producing datasets that have a Spearman correlation at least\n    as extreme as the one computed from these datasets. The p-values\n    are not entirely reliable but are probably reasonable for datasets\n    larger than 500 or so.\n\n    Parameters\n    ----------\n    array_1 : pandas series\n        Series containing multiple observations\n    array_2 : pandas series\n        Series containing multiple observations\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n        * 'two-sided': the correlation is nonzero\n        * 'less': the correlation is negative (less than zero)\n        * 'greater':  the correlation is positive (greater than zero)\n\n    Returns\n    -------\n    correlation : float or ndarray (2-D square)\n        Spearman correlation matrix or correlation coefficient (if only 2\n        variables are given as parameters. Correlation matrix is square with\n        length equal to total number of variables (columns or rows) in ``a``\n        and ``b`` combined.\n    pvalue : float\n        The p-value for a hypothesis test whose null hypotheisis\n        is that two sets of data are uncorrelated. See `alternative` above\n        for alternative hypotheses. `pvalue` has the same\n        shape as `correlation`.\n\n    References\n    -------------\n        [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n        Probability and Statistics Tables and Formulae. Chapman & Hall: New\n        York. 2000.\n        Section  14.7\n\n    Examples\n    --------\n    >>> spearmanr([1,2,3,4,5], [5,6,7,8,7])\n    SpearmanrResult(correlation=0.82078..., pvalue=0.08858...)\n    \"\"\"\n\n    return stats.spearmanr(array_1, array_2, alternative=\"two-sided\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# report_generator.py\n\nfrom docxtpl import DocxTemplate, InlineImage\n\n\ndef get_context():\n    return {\n        'invoice_no': 12345,\n        'date': '30 Mar',\n        'due_date': '30 Apr',\n        'name': 'Jane Doe',\n        'address': '123 Quiet Lane',\n        'subtotal': 335,\n        'tax_amt': 10,\n        'total': 345,\n        'amt_paid': 100,\n        'amt_due': 245,\n        'row_contents': [\n            {\n                'description': 'Eggs',\n                'quantity': 30,\n                'rate': 5,\n                'amount': 150\n            }, {\n                'description': 'All Purpose Flour',\n                'quantity': 10,\n                'rate': 15,\n                'amount': 150\n            }, {\n                'description': 'Eggs',\n                'quantity': 5,\n                'rate': 7,\n                'amount': 35\n            }\n        ]\n    }\n\n\ndoc = DocxTemplate('monitoring_report_template.docx')\ncontext = get_context()\n\ndoc.render(context)\ndoc.save(\"generated_doc.docx\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# monitoring.py\n\n\"\"\"\nScorecard monitoring (System stability report)\n\"\"\"\n\nimport numbers\nimport time\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport numpy as np\nimport pandas as pd\n\nfrom scipy import stats\nfrom sklearn.base import BaseEstimator\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom ..binning.binning_statistics import bin_str_format\nfrom ..binning.metrics import jeffrey\nfrom ..binning.prebinning import PreBinning\nfrom ..formatting import dataframe_to_string\nfrom ..logging import Logger\nfrom ..metrics.classification import gini\nfrom ..metrics.classification import imbalanced_classification_metrics\nfrom ..metrics.regression import regression_metrics\nfrom .monitoring_information import print_monitoring_information\nfrom .scorecard import Scorecard\n\n\nlogger = Logger(__name__).logger\n\n\nPSI_VERDICT_MSG = {0: \"No significant change\",\n                   1: \"Requires investigation\",\n                   2: \"Significance change\"}\n\n\ndef _check_parameters(scorecard, psi_method, psi_n_bins,\n                      psi_min_bin_size, show_digits, verbose):\n\n    if not isinstance(scorecard, Scorecard):\n        raise TypeError(\"scorecard must be a Scorecard instance.\")\n\n    if psi_method not in (\"uniform\", \"quantile\", \"cart\"):\n        raise ValueError('Invalid value for prebinning_method. Allowed '\n                         'string values are \"cart\", \"quantile\" and '\n                         '\"uniform\".')\n\n    if psi_n_bins is not None:\n        if (not isinstance(psi_n_bins, numbers.Integral) or\n                psi_n_bins <= 0):\n            raise ValueError(\"psi_n_bins must be a positive integer; got {}.\"\n                             .format(psi_n_bins))\n\n    if psi_min_bin_size is not None:\n        if (not isinstance(psi_min_bin_size, numbers.Number) or\n                not 0. < psi_min_bin_size <= 0.5):\n            raise ValueError(\"psi_min_bin_size must be in (0, 0.5]; got {}.\"\n                             .format(psi_min_bin_size))\n\n    if (not isinstance(show_digits, numbers.Integral) or\n            not 0 <= show_digits <= 8):\n        raise ValueError(\"show_digits must be an integer in [0, 8]; \"\n                         \"got {}.\".format(show_digits))\n\n    if not isinstance(verbose, bool):\n        raise TypeError(\"verbose must be a boolean; got {}.\".format(verbose))\n\n\ndef print_psi_report(df_psi):\n    t_psi = df_psi.PSI.values[-1]\n    psi = df_psi.PSI.values[:-1]\n\n    splits = [0.1, 0.25]\n    n_bins = len(splits) + 1\n    indices = np.digitize(psi, splits, right=True)\n\n    psi_bins = np.empty(n_bins).astype(np.int64)\n    for i in range(n_bins):\n        mask = (indices == i)\n        psi_bins[i] = len(psi[mask])\n\n    p_psi_bins = psi_bins / psi_bins.sum()\n    psi_verdict = PSI_VERDICT_MSG[np.digitize(t_psi, splits)]\n\n    df_psi_string = dataframe_to_string(pd.DataFrame({\n        \"PSI bin\": [\"[0.00, 0.10)\", \"[0.10, 0.25)\", \"[0.25, Inf+)\"],\n        \"Count\": psi_bins,\n        \"Count (%)\": p_psi_bins\n        }), tab=4)\n\n    psi_stats = (\n        \"  Population Stability Index (PSI)\\n\\n\"\n        \"\"\n        \"\\n\"\n        \"    PSI total:     {:>7.4f} ({})\\n\\n\"\n        \"{}\\n\").format(t_psi, psi_verdict, df_psi_string)\n\n    print(psi_stats)\n\n\ndef print_tests_report(df_tests):\n    pvalues = df_tests[\"p-value\"].values\n\n    splits = [0.05, 0.1, 0.5]\n    n_bins = len(splits) + 1\n    indices = np.digitize(pvalues, splits, right=True)\n\n    pvalue_bins = np.empty(n_bins).astype(np.int64)\n    for i in range(n_bins):\n        mask = (indices == i)\n        pvalue_bins[i] = len(pvalues[mask])\n\n    p_pvalue_bins = pvalue_bins / pvalue_bins.sum()\n\n    df_tests_string = dataframe_to_string(pd.DataFrame({\n        \"p-value bin\":  [\"[0.00, 0.05)\", \"[0.05, 0.10)\", \"[0.10, 0.50)\",\n                         \"[0.50, 1.00)\"],\n        \"Count\": pvalue_bins,\n        \"Count (%)\": p_pvalue_bins,\n        }), tab=4)\n\n    tests_stats = (\n        \"  Significance tests (H0: actual == expected)\\n\\n\"\n        \"{}\\n\").format(df_tests_string)\n\n    print(tests_stats)\n\n\ndef print_target_report(df_target):\n    df_target_string = dataframe_to_string(df_target, tab=4)\n\n    target_stats = (\n        \"  Target analysis\\n\\n\"\n        \"{}\\n\").format(df_target_string)\n\n    print(target_stats)\n\n\ndef print_performance_report(df_performance):\n    df_performance_string = dataframe_to_string(df_performance, tab=4)\n\n    performance_stats = (\n        \"  Performance metrics\\n\\n\"\n        \"{}\\n\".format(df_performance_string)\n        )\n\n    print(performance_stats)\n\n\ndef print_system_report(df_psi, df_tests, df_target_analysis, df_performance):\n\n    print(\"-----------------------------------\\n\"\n          \"Monitoring: System Stability Report\\n\"\n          \"-----------------------------------\\n\")\n\n    print_psi_report(df_psi)\n    print_tests_report(df_tests)\n    print_target_report(df_target_analysis)\n    print_performance_report(df_performance)\n\n\nclass ScorecardMonitoring(BaseEstimator):\n    \"\"\"Scorecard monitoring.\n    Parameters\n    ----------\n    scorecard : object\n        A ``Scorecard`` fitted instance.\n    psi_method : str, optional (default=\"cart\")\n        The binning method to compute the Population Stability Index (PSI).\n        Supported methods are \"cart\" for a CART\n        decision tree, \"quantile\" to generate prebins with approximately same\n        frequency and \"uniform\" to generate prebins with equal width. Method\n        \"cart\" uses `sklearn.tree.DecistionTreeClassifier\n        <https://scikit-learn.org/stable/modules/generated/sklearn.tree.\n        DecisionTreeClassifier.html>`_.\n    psi_n_bins : int (default=20)\n        The maximum number of bins to compute PSI.\n    psi_min_bin_size : float (default=0.05)\n        The fraction of mininum number of records for PSI bin.\n    show_digits : int, optional (default=2)\n        The number of significant digits of the bin column.\n    verbose : bool (default=False)\n        Enable verbose output.\n    \"\"\"\n    def __init__(self, scorecard, psi_method=\"cart\", psi_n_bins=20,\n                 psi_min_bin_size=0.05, show_digits=2, verbose=False):\n\n        self.scorecard = scorecard\n\n        self.psi_method = psi_method\n        self.psi_n_bins = psi_n_bins\n        self.psi_min_bin_size = psi_min_bin_size\n\n        self.show_digits = show_digits\n        self.verbose = verbose\n\n        # auxiliary data\n        self._splits = None\n        self._df_psi = None\n        self._df_tests = None\n        self._target_dtype = None\n        self._n_records_a = None\n        self._n_records_e = None\n        self._metric_a = None\n        self._metric_e = None\n\n        # time\n        self._time_total = None\n        self._time_system = None\n        self._time_variables = None\n\n        # flags\n        self._is_fitted = False\n\n    def fit(self, X_actual, y_actual, X_expected, y_expected):\n        \"\"\"Fit monitoring with actual and expected data.\n        Parameters\n        ----------\n        X_actual : pandas.DataFrame\n            New/actual/test data input samples.\n        y_actual : array-like of shape (n_samples,)\n            Target vector relative to X actual.\n        X_expected : pandas.DataFrame\n            Trainning data used for fitting the scorecard.\n        y_expected : array-like of shape (n_samples,)\n            Target vector relative to X expected.\n        Returns\n        -------\n        self : ScorecardMonitoring\n            Fitted monitoring.\n        \"\"\"\n        time_init = time.perf_counter()\n\n        if self.verbose:\n            logger.info(\"Monitoring started.\")\n            logger.info(\"Options: check parameters.\")\n\n        # Check parameters\n        _check_parameters(**self.get_params(deep=False))\n\n        # Check if scorecard is fitted\n        self.scorecard._check_is_fitted()\n\n        target_dtype = type_of_target(y_actual)\n        target_dtype_e = type_of_target(y_expected)\n\n        if target_dtype not in (\"binary\", \"continuous\"):\n            raise ValueError(\"Target type (actual) {} is not supported.\"\n                             .format(target_dtype))\n\n        if target_dtype_e not in (\"binary\", \"continuous\"):\n            raise ValueError(\"Target type (expected) {} is not supported.\"\n                             .format(target_dtype_e))\n\n        if target_dtype != target_dtype_e:\n            raise ValueError(\"Target types must coincide; {} != {}.\"\n                             .format(target_dtype, target_dtype_e))\n\n        self._target_dtype = target_dtype\n\n        # Check variable names\n        if list(X_actual.columns) != list(X_expected.columns):\n            raise ValueError(\"Dataframes X_actual and X_expected must \"\n                             \"have the same columns.\")\n\n        # Statistics at system level\n        if self.verbose:\n            logger.info(\"System stability analysis started.\")\n\n        time_system = time.perf_counter()\n        self._fit_system(X_actual, y_actual, X_expected, y_expected)\n        self._time_system = time.perf_counter() - time_system\n\n        if self.verbose:\n            logger.info(\"System stability analysis terminated. Time: {:.4f}s\"\n                        .format(self._time_system))\n\n        # Statistics at variable level\n        if self.verbose:\n            logger.info(\"Variable analysis started.\")\n\n        time_variable = time.perf_counter()\n        self._fit_variables(X_actual, X_expected)\n        self._time_variable = time.perf_counter() - time_variable\n\n        if self.verbose:\n            logger.info(\"Variable analysis terminated. Time: {:.4f}s\"\n                        .format(self._time_variable))\n\n        self._time_total = time.perf_counter() - time_init\n\n        if self.verbose:\n            logger.info(\"Monitoring terminated. Time: {:.4f}s\"\n                        .format(self._time_total))\n\n        # Completed successfully\n        self._is_fitted = True\n\n        return self\n\n    def information(self, print_level=1):\n        \"\"\"Print overview information about the options settings and\n        statistics.\n        Parameters\n        ----------\n        print_level : int (default=1)\n            Level of details.\n        \"\"\"\n        self._check_is_fitted()\n\n        if not isinstance(print_level, numbers.Integral) or print_level < 0:\n            raise ValueError(\"print_level must be an integer >= 0; got {}.\"\n                             .format(print_level))\n\n        n_vars = np.count_nonzero(self.scorecard.binning_process_._support)\n        dict_user_options = self.get_params(deep=False)\n\n        print_monitoring_information(print_level, self._n_records_a.sum(),\n                                     self._n_records_e.sum(), n_vars,\n                                     self._target_dtype, self._time_total,\n                                     self._time_system,\n                                     self._time_variable,\n                                     dict_user_options)\n\n    def system_stability_report(self):\n        \"\"\"Print overview information and statistics about system stability.\n        It includes qualitative suggestions regarding the necessity of\n        scorecard updates.\n        \"\"\"\n        self._check_is_fitted()\n\n        print_system_report(self._df_psi, self._df_tests,\n                            self._df_target_analysis, self._df_performance)\n\n    def psi_table(self):\n        \"\"\"System Population Stability Index (PSI) table.\n        Returns\n        -------\n        psi_table : pandas.DataFrame\n        \"\"\"\n        self._check_is_fitted()\n\n        return self._df_psi\n\n    def psi_variable_table(self, name=None, style=\"summary\"):\n        \"\"\"Population Stability Index (PSI) at variable level.\n        Parameters\n        ----------\n        name : str or None (default=None)\n            The variable name. If name is None, a table with all variables\n            is returned.\n        style : str, optional (default=\"summary\")\n            Supported styles are \"summary\" and \"detailed\". Summary only\n            includes the total PSI for each variable. Detailed includes the\n            PSI for each variable at bin level.\n        Returns\n        -------\n        psi_table : pandas.DataFrame\n        \"\"\"\n        self._check_is_fitted()\n\n        if style not in (\"summary\", \"detailed\"):\n            raise ValueError('Invalid value for style. Allowed string '\n                             'values are \"summary\" and \"detailed\".')\n\n        if name is not None:\n            variables = self.scorecard.binning_process_.get_support(names=True)\n\n            if name not in variables:\n                raise ValueError(\"name {} does not match a binned variable \"\n                                 \"included in the provided scorecard.\"\n                                 .format(name))\n\n            dv = self._df_psi_variable[self._df_psi_variable.Variable == name]\n\n            if style == \"summary\":\n                return dv.groupby([\"Variable\"])[\"PSI\"].sum()\n            else:\n                return dv\n\n        if style == \"summary\":\n            return pd.DataFrame(\n                self._df_psi_variable.groupby(['Variable'])['PSI'].sum()\n                ).reset_index()\n        elif style == \"detailed\":\n            return self._df_psi_variable\n\n    def tests_table(self):\n        \"\"\"Compute statistical tests to determine if event rate (Chi-square\n        test - binary target) or mean (Student's t-test - continuous target)\n        are significantly different. Null hypothesis (actual == expected).\n        Returns\n        -------\n        tests_table : pandas.DataFrame\n        \"\"\"\n        self._check_is_fitted()\n\n        return self._df_tests\n\n    def psi_plot(self, savefig=None):\n        \"\"\"Plot Population Stability Index (PSI).\n        Parameters\n        ----------\n        savefig : str or None (default=None)\n            Path to save the plot figure.\n        \"\"\"\n        self._check_is_fitted()\n\n        fig, ax1 = plt.subplots()\n\n        n_bins = len(self._n_records_a)\n        indices = np.arange(n_bins)\n        width = np.min(np.diff(indices))/3\n\n        p_records_a = self._n_records_a / self._n_records_a.sum() * 100.0\n        p_records_e = self._n_records_e / self._n_records_e.sum() * 100.0\n\n        p1 = ax1.bar(indices-width, p_records_a, width, color='tab:red',\n                     label=\"Records Actual\", alpha=0.75)\n        p2 = ax1.bar(indices, p_records_e, width, color='tab:blue',\n                     label=\"Records Expected\", alpha=0.75)\n\n        handles = [p1[0], p2[0]]\n        labels = ['Actual', 'Expected']\n\n        ax1.set_xlabel(\"Bin ID\", fontsize=12)\n        ax1.set_ylabel(\"Population distribution\", fontsize=13)\n        ax1.yaxis.set_major_formatter(mtick.PercentFormatter())\n\n        ax2 = ax1.twinx()\n\n        if self._target_dtype == \"binary\":\n            metric_label = \"Event rate\"\n        elif self._target_dtype == \"continuous\":\n            metric_label = \"Mean\"\n\n        ax2.plot(indices, self._metric_a, linestyle=\"solid\", marker=\"o\",\n                 color='tab:red')\n        ax2.plot(indices, self._metric_e,  linestyle=\"solid\", marker=\"o\",\n                 color='tab:blue')\n\n        ax2.set_ylabel(metric_label, fontsize=13)\n        ax2.xaxis.set_major_locator(mtick.MultipleLocator(1))\n\n        ax2.set_xlim(-width * 2, n_bins - width * 2)\n\n        plt.legend(handles, labels, loc=\"upper center\",\n                   bbox_to_anchor=(0.5, -0.2), ncol=2, fontsize=12)\n\n        if savefig is None:\n            plt.show()\n        else:\n            if not isinstance(savefig, str):\n                raise TypeError(\"savefig must be a string path; got {}.\"\n                                .format(savefig))\n            plt.savefig(savefig)\n            plt.close()\n\n    def _fit_system(self, X_actual, y_actual, X_expected, y_expected):\n        if self._target_dtype == \"binary\":\n            problem_type = \"classification\"\n        else:\n            problem_type = \"regression\"\n\n        score_actual = self.scorecard.score(X_actual)\n        score_expected = self.scorecard.score(X_expected)\n\n        prebinning = PreBinning(problem_type=problem_type,\n                                method=self.psi_method,\n                                n_bins=self.psi_n_bins,\n                                min_bin_size=self.psi_min_bin_size\n                                ).fit(score_expected, y_expected)\n\n        splits = prebinning.splits\n        n_bins = len(splits) + 1\n\n        # Compute basic metrics\n        indices_a = np.digitize(score_actual, splits, right=True)\n        indices_e = np.digitize(score_expected, splits, right=True)\n\n        if self._target_dtype == \"binary\":\n            n_nonevent_a = np.empty(n_bins).astype(np.int64)\n            n_event_a = np.empty(n_bins).astype(np.int64)\n            n_nonevent_e = np.empty(n_bins).astype(np.int64)\n            n_event_e = np.empty(n_bins).astype(np.int64)\n\n            y0_a = (y_actual == 0)\n            y1_a = ~ y0_a\n\n            y0_e = (y_expected == 0)\n            y1_e = ~ y0_e\n\n            for i in range(n_bins):\n                mask_a = (indices_a == i)\n                n_nonevent_a[i] = np.count_nonzero(y0_a & mask_a)\n                n_event_a[i] = np.count_nonzero(y1_a & mask_a)\n\n                mask_e = (indices_e == i)\n                n_nonevent_e[i] = np.count_nonzero(y0_e & mask_e)\n                n_event_e[i] = np.count_nonzero(y1_e & mask_e)\n\n            n_records_a = n_nonevent_a + n_event_a\n            n_records_e = n_nonevent_e + n_event_e\n        else:\n            n_records_a = np.empty(n_bins).astype(np.int64)\n            n_records_e = np.empty(n_bins).astype(np.int64)\n            mean_a = np.empty(n_bins)\n            mean_e = np.empty(n_bins)\n            std_a = np.empty(n_bins)\n            std_e = np.empty(n_bins)\n\n            for i in range(n_bins):\n                mask_a = (indices_a == i)\n                n_records_a[i] = np.count_nonzero(mask_a)\n                mean_a[i] = y_actual[mask_a].mean()\n                std_a[i] = y_actual[mask_a].std()\n\n                mask_e = (indices_e == i)\n                n_records_e[i] = np.count_nonzero(mask_e)\n                mean_e[i] = y_expected[mask_e].mean()\n                std_e[i] = y_expected[mask_e].std()\n\n        bins = np.concatenate([[-np.inf], splits, [np.inf]])\n        bin_str = bin_str_format(bins, self.show_digits)\n\n        # Target analysis\n        if self._target_dtype == \"binary\":\n            self._system_target_binary(n_records_a, n_event_a, n_nonevent_a,\n                                       n_records_e, n_event_e, n_nonevent_e)\n        else:\n            self._system_target_continuous(y_actual, y_expected)\n\n        # Population Stability Information (PSI)\n        self._system_psi(bin_str, n_records_a, n_records_e)\n\n        # Significance tests\n        if self._target_dtype == \"binary\":\n            self._system_tests_binary(\n                bin_str, n_records_a, n_event_a, n_nonevent_a,\n                n_records_e, n_event_e, n_nonevent_e)\n        else:\n            self._system_tests_continuous(\n                bin_str, n_records_a, mean_a, std_a,\n                n_records_e, mean_e, std_e)\n\n        # Performance analysis\n        if self._target_dtype == \"binary\":\n            self._system_performance_binary(\n                X_actual, y_actual, X_expected, y_expected)\n        else:\n            self._system_performance_continuous(\n                X_actual, y_actual, X_expected, y_expected)\n\n        self._splits = splits\n        self._n_records_a = n_records_a\n        self._n_records_e = n_records_e\n\n    def _system_psi(self, bin_str, n_records_a, n_records_e):\n        t_n_records_a = n_records_a.sum()\n        t_n_records_e = n_records_e.sum()\n        p_records_a = n_records_a / t_n_records_a\n        p_records_e = n_records_e / t_n_records_e\n\n        psi = jeffrey(p_records_a, p_records_e, return_sum=False)\n\n        df_psi = pd.DataFrame({\n            \"Bin\": bin_str,\n            \"Count A\": n_records_a,\n            \"Count E\": n_records_e,\n            \"Count A (%)\": p_records_a,\n            \"Count E (%)\": p_records_e,\n            \"PSI\": psi\n            })\n\n        totals = [\"\", t_n_records_a, t_n_records_e, 1, 1, psi.sum()]\n        df_psi.loc[\"Totals\"] = totals\n\n        self._df_psi = df_psi\n\n    def _system_tests_binary(self, bin_str, n_records_a, n_event_a,\n                             n_nonevent_a, n_records_e, n_event_e,\n                             n_nonevent_e):\n        t_statistics = []\n        p_values = []\n\n        n_bins = len(bin_str)\n        event_rate_a = n_event_a / n_records_a\n        event_rate_e = n_event_e / n_records_e\n\n        self._metric_a = event_rate_a\n        self._metric_e = event_rate_e\n\n        for i in range(n_bins):\n            obs = np.array([\n                [n_nonevent_a[i], n_nonevent_e[i]],\n                [n_event_a[i], n_event_e[i]]])\n\n            t, p, _, _ = stats.chi2_contingency(obs, correction=False)\n\n            t_statistics.append(t)\n            p_values.append(p)\n\n        df_tests = pd.DataFrame({\n            \"Bin\": bin_str,\n            \"Count A\": n_records_a,\n            \"Count E\": n_records_e,\n            \"Event rate A\": event_rate_a,\n            \"Event rate E\": event_rate_e,\n            \"statistic\": t_statistics,\n            \"p-value\": p_values\n            })\n\n        self._df_tests = df_tests\n\n    def _system_tests_continuous(self, bin_str, n_records_a, mean_a, std_a,\n                                 n_records_e, mean_e, std_e):\n\n        self._metric_a = mean_a\n        self._metric_e = mean_e\n\n        t_statistics = []\n        p_values = []\n\n        n_bins = len(bin_str)\n        for i in range(n_bins):\n            t, p = stats.ttest_ind_from_stats(\n                mean_a[i], std_a[i], n_records_a[i],\n                mean_e[i], std_e[i], n_records_e[i], False)\n\n            t_statistics.append(t)\n            p_values.append(p)\n\n        df_tests = pd.DataFrame({\n            \"Bin\": bin_str,\n            \"Count A\": n_records_a,\n            \"Count E\": n_records_e,\n            \"Mean A\": mean_a,\n            \"Mean E\": mean_e,\n            \"Std A\": std_a,\n            \"Std E\": std_e,\n            \"statistic\": t_statistics,\n            \"p-value\": p_values\n            })\n\n        self._df_tests = df_tests\n\n    def _system_target_binary(self, n_records_a, n_event_a, n_nonevent_a,\n                              n_records_e, n_event_e, n_nonevent_e):\n\n        t_n_records_a = n_records_a.sum()\n        t_n_event_a = n_event_a.sum()\n        t_n_nonevent_a = n_nonevent_a.sum()\n\n        t_n_records_e = n_records_e.sum()\n        t_n_event_e = n_event_e.sum()\n        t_n_nonevent_e = n_nonevent_e.sum()\n\n        event_rate_a = t_n_event_a / t_n_records_a\n        event_rate_e = t_n_event_e / t_n_records_e\n\n        df_target = pd.DataFrame({\n            \"Metric\": [\"Number of records\", \"Event records\",\n                       \"Non-event records\"],\n            \"Actual\": [t_n_records_a, t_n_event_a, t_n_nonevent_a],\n            \"Actual (%)\": [\"-\", event_rate_a, 1 - event_rate_a],\n            \"Expected\": [t_n_records_e, t_n_event_e, t_n_nonevent_e],\n            \"Expected (%)\": [\"-\", event_rate_e, 1 - event_rate_e]\n            })\n\n        self._df_target_analysis = df_target\n\n    def _system_target_continuous(self, y_actual, y_expected):\n\n        mean_a = y_actual.mean()\n        mean_e = y_expected.mean()\n        std_a = y_actual.std()\n        std_e = y_expected.std()\n\n        p25_a, median_a, p75_a = np.percentile(y_actual, [25, 50, 75])\n        p25_e, median_e, p75_e = np.percentile(y_expected, [25, 50, 75])\n\n        df_target = pd.DataFrame({\n            \"Metric\": [\"Mean\", \"Std\", \"p25\", \"Median\", \"p75\"],\n            \"Actual\": [mean_a, std_a, p25_a, median_a, p75_a],\n            \"Expected\": [mean_e, std_e, p25_e, median_e, p75_e]\n            })\n\n        self._df_target_analysis = df_target\n\n    def _system_performance_binary(self, X_actual, y_actual, X_expected,\n                                   y_expected):\n        # Metrics derived from confusion matrix\n        y_true_a = y_actual\n        y_pred_a = self.scorecard.predict(X_actual)\n        d_metrics_a = imbalanced_classification_metrics(y_true_a, y_pred_a)\n\n        y_true_e = y_expected\n        y_pred_e = self.scorecard.predict(X_expected)\n        d_metrics_e = imbalanced_classification_metrics(y_true_e, y_pred_e)\n\n        metric_names = list(d_metrics_a.keys())\n        metrics_a = list(d_metrics_a.values())\n        metrics_e = list(d_metrics_e.values())\n\n        # Gini\n        y_pred_proba_a = self.scorecard.predict_proba(X_actual)[:, 1]\n        gini_a = gini(y_true_a, y_pred_proba_a)\n\n        y_pred_proba_e = self.scorecard.predict_proba(X_expected)[:, 1]\n        gini_e = gini(y_true_e, y_pred_proba_e)\n\n        metric_names.append(\"Gini\")\n        metrics_a.append(gini_a)\n        metrics_e.append(gini_e)\n\n        diff = np.array(metrics_a) - np.array(metrics_e)\n\n        df_performance = pd.DataFrame({\n            \"Metric\": metric_names,\n            \"Actual\": metrics_a,\n            \"Expected\": metrics_e,\n            \"Diff A - E\": diff,\n            })\n\n        self._df_performance = df_performance\n\n    def _system_performance_continuous(self, X_actual, y_actual, X_expected,\n                                       y_expected):\n        y_true_a = y_actual\n        y_pred_a = self.scorecard.predict(X_actual)\n        d_metrics_a = regression_metrics(y_true_a, y_pred_a)\n\n        y_true_e = y_expected\n        y_pred_e = self.scorecard.predict(X_expected)\n        d_metrics_e = regression_metrics(y_true_e, y_pred_e)\n\n        metric_names = list(d_metrics_a.keys())\n        metrics_a = list(d_metrics_a.values())\n        metrics_e = list(d_metrics_e.values())\n\n        diff = np.array(metrics_a) - np.array(metrics_e)\n\n        df_performance = pd.DataFrame({\n            \"Metric\": metric_names,\n            \"Actual\": metrics_a,\n            \"Expected\": metrics_e,\n            \"Diff A - E\": diff,\n            })\n\n        self._df_performance = df_performance\n\n    def _fit_variables(self, X_actual, X_expected):\n        variables = self.scorecard.binning_process_.get_support(names=True)\n        sc_table = self.scorecard.table()\n\n        l_df_psi = []\n\n        for name in variables:\n            optb = self.scorecard.binning_process_.get_binned_variable(name)\n            ta = optb.transform(X_actual[name], metric=\"indices\")\n            te = optb.transform(X_expected[name], metric=\"indices\")\n\n            n_bins = te.max() + 1\n\n            n_records_a = np.empty(n_bins).astype(np.int64)\n            n_records_e = np.empty(n_bins).astype(np.int64)\n\n            for i in range(n_bins):\n                n_records_a[i] = np.count_nonzero(ta == i)\n                n_records_e[i] = np.count_nonzero(te == i)\n\n            t_n_records_a = n_records_a.sum()\n            t_n_records_e = n_records_e.sum()\n            p_records_a = n_records_a / t_n_records_a\n            p_records_e = n_records_e / t_n_records_e\n\n            psi = jeffrey(p_records_a, p_records_e, return_sum=False)\n            bins = sc_table[sc_table.Variable == name][\"Bin\"].values[:n_bins]\n\n            df_psi = pd.DataFrame({\n                \"Variable\": [name] * n_bins,\n                \"Bin\": bins,\n                \"Count A\": n_records_a,\n                \"Count E\": n_records_e,\n                \"Count A (%)\": p_records_a,\n                \"Count E (%)\": p_records_e,\n                \"PSI\": psi\n                })\n\n            l_df_psi.append(df_psi)\n\n        self._df_psi_variable = pd.concat(l_df_psi)\n        self._df_psi_variable.reset_index()\n\n    def _check_is_fitted(self):\n        if not self._is_fitted:\n            raise NotFittedError(\"This {} instance is not fitted yet. Call \"\n                                 \"'fit' with appropriate arguments.\"\n                                 .format(self.__class__.__name__))\n\n    @property\n    def psi_splits(self):\n        \"\"\"List of splits points used to compute system PSI.\n        Returns\n        -------\n        splits : numpy.ndarray\n        \"\"\"\n        self._check_is_fitted()\n\n        return self._splits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main.py -- old version\n\n# # # Database\n# # import oracledb\n# # import sqlite3\n# # import sqlalchemy\n# # # Data manipulation\n# # import pandas as pd\n# # from helper_functions import col_dropper, fix_dtypes, DataFrameImputer, AutoPrepareScoreCard\n# # import numpy as np\n# # from optbinning import BinningProcess\n# # from optbinning import Scorecard\n# # from optbinning.scorecard import ScorecardMonitoring\n# #\n# # import joblib\n# #\n# # con = sqlite3.connect('credit_data.db')\n# # cur = con.cursor()\n# #\n# #\n# # def read_pipeline(path):  # __REDO - for different model formats\n# #     pipeline = joblib.load(path)\n# #     return pipeline\n# #\n# #\n# # estimator = read_pipeline('clf_pipeline.pkl')\n# #\n# #\n# #\n# # def list_all_table():\n# #     query = \"\"\"SELECT name FROM sqlite_master WHERE type='table';\"\"\"\n# #     print(pd.read_sql(query, con, index_col=None))\n# #     return None\n# #\n# #\n# # query_inc = \"\"\"SELECT *\n# # FROM test_set\n# # \"\"\"\n# # test_set = pd.read_sql(query_inc, con, index_col=None)\n# #\n# # scorecard = Scorecard()\n#\n#\n# import datetime\n# import dash_bootstrap_components as dbc\n# from dash import html, Dash\n#\n# app = Dash(__name__, external_stylesheets=[dbc.themes.CYBORG])\n#\n# # app.layout = html.H1('The time is: ' + str(datetime.datetime.now()))\n# #\n# # if __name__ == '__main__':\n# #     app.run_server(debug=True)\n#\n# def serve_layout():\n#     return html.H1('The time is: ' + str(datetime.datetime.now()))\n#\n# app.layout = serve_layout\n#\n# if __name__ == '__main__':\n#     app.run_server(debug=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dashboard.py --old version\n\nfrom dash import Dash, dcc, html, Input, Output\nimport dash_bootstrap_components as dbc\nimport plotly.express as px\n\nfrom plotting_functions import plot_hist_dist, plot_feature_importances, create_card, psi_plot_ly, plot_auc_roc, plot_ks, psi_variable_plot\nfrom helper_functions import col_dropper, fix_dtypes, DataFrameImputer, AutoPrepareScoreCard\n# import plotly.graph_objects as go\nimport pandas as pd\n# from datetime import date\nimport sqlite3\nfrom sklearn.metrics import roc_curve, roc_auc_score, log_loss\nfrom sklearn.model_selection import train_test_split\nimport json\nimport helper_functions\nimport joblib\n\nfrom optbinning import BinningProcess\nfrom optbinning.scorecard import ScorecardMonitoring\n\nimport credit_py_validation as cpv\n\nfrom plotly.tools import mpl_to_plotly\n\n# Global variables\nLOGO_PATH = 'assets/halyk_logo.png'\n\n# Database connection\ncon = sqlite3.connect('new_credit_data.db')\ncur = con.cursor()\nquery_X = \"\"\"SELECT *\nFROM X_train_log\n\"\"\"\nX_train_log = pd.read_sql(query_X, con, index_col=None)\n\nquery_y = \"\"\"SELECT *\nFROM y_train_log\n\"\"\"\ny_train_log = pd.read_sql(query_y, con, index_col=None)\nX_train, X_test, y_train, y_test = train_test_split(X_train_log, y_train_log, test_size=0.2, random_state=42)\ndef do_calculations():\n    # Read binning fit params\n    with open('binning_fit_params.json') as json_file:\n        binning_fit_params = json.load(json_file)\n\n    # Loading the model\n    lr_model = joblib.load('lr_model.pkl')\n\n    # selection_criteria = {\n    #     \"iv\": {\"min\": 0.02, \"max\": 1},\n    #     \"quality_score\": {\"min\": 0.01}\n    # }\n    variable_names = list(X_train.columns)\n\n    binning_process = BinningProcess(variable_names,  # selection_criteria=selection_criteria,\n                                     binning_fit_params=binning_fit_params)\n\n    scorecard = helper_functions.AutoPrepareScoreCard(binning_process=binning_process,\n                                                      estimator=lr_model, scaling_method=\"min_max\",\n                                                      scaling_method_params={\"min\": 300, \"max\": 850}, verbose=True)\n\n    scorecard.fit(X_train, y_train.values.ravel())\n\n    monitoring = ScorecardMonitoring(scorecard=scorecard, psi_method=\"cart\",\n                                     psi_n_bins=10, verbose=True)\n\n    monitoring.fit(X_test, y_test.values.ravel(), X_train, y_train.values.ravel())\n\n    monitoring.psi_table()\n\n    # Calculate train\n    pd_X_train = scorecard.predict_proba(X_train)[:, 1]\n    default_flag_X_train = scorecard.predict(X_train)\n    score_X_train = scorecard.score(X_train)\n    rating_X_train = scorecard.get_credit_ratings(score_X_train)\n\n    X_train['Default Probability'] = pd_X_train\n    X_train['Credit Rating'] = rating_X_train\n    X_train['Default Flag'] = default_flag_X_train\n\n    # Calculate test\n    pd_X_test = scorecard.predict_proba(X_test)[:, 1]\n    default_flag_X_test = scorecard.predict(X_test)\n    score_X_test = scorecard.score(X_test)\n    rating_X_test = scorecard.get_credit_ratings(score_X_test)\n\n    X_test['Default Probability'] = pd_X_test\n    X_test['Credit Rating'] = rating_X_test\n    X_test['Default Flag'] = default_flag_X_test\n\n    incoming_batch_results = helper_functions.conduct_tests(X_test)\n    test_results = helper_functions.conduct_tests(X_train)\n    # card_tests = ['log_loss', 'brier', 'roc_auc', 'ber']\n    return incoming_batch_results, test_results, scorecard, monitoring\n\n# # plot_auc_roc(y_train, scorecard.predict_proba(X_train)[:, 0])))\n# def get_change(current, previous):\n#     if current == previous:\n#         return 0\n#     try:\n#         return (abs(current - previous) / previous) * 100.0\n#     except ZeroDivisionError:\n#         return float('inf')\n# tests_percent_change = {k + '_change': get_change(test_results[k], incoming_batch_results[k]) for k in card_tests}\n\n\n\napp = Dash(__name__, external_stylesheets=[dbc.themes.CYBORG])\n\n# app.layout = html.Div([\n#     html.H2('Welcome to HB Credit Risk Model Monitoring System!'),\n#     html.H3('Please, choose a model from the dropdown menu below: '),\n#     # html.Div(dcc.dropdown)\n#     html.Button(id='calculate-button-state', n_clicks=0, children='Submit')\n# ])\n\n###### RE-WORK - initilize arrays and values for empty DASHBOARD!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\nincoming_batch_results, test_results, scorecard, monitoring = do_calculations() # CHANGE TO PRODUCE PLACEHOLDERS\n\n# @app.callback(\n#     Output('graph-distributions', 'figure'),\n#     Input('dropdown', 'value')\n# )\n# def update_layout(value):\n#     fig = plot_hist_dist(train[value], test[value])\n#     return fig\n\n\napp.layout = html.Div([\n    # date_range(),\n    html.Br(),\n    html.Div(id='output-container-date-picker-range'),\n    dbc.Row([html.Img(src=LOGO_PATH, style={'height': '10%', 'width': '10%'})]),\n    html.H2('Welcome to HB Credit Risk Model Monitoring System!'),\n    html.Br(),\n\n    dcc.Tabs([\n        dcc.Tab(label='Assess accuracy', children=[\n            dbc.Row(\n                [dbc.Col(create_card(incoming_batch_results['log_loss'], '1', 'Log Loss',\n                                     f'Initial: {test_results[\"log_loss\"]:.3f}')),\n                 dbc.Col(\n                     create_card(incoming_batch_results['brier'], '2', 'Brier',\n                                 f'Initial: {test_results[\"brier\"]:.3f}')),\n                 dbc.Col(\n                     create_card(incoming_batch_results['roc_auc'], '3', 'AUROC',\n                                 f'Initial: {test_results[\"roc_auc\"]:.3f}')),\n                 dbc.Col(create_card(incoming_batch_results['ber'], '4', 'BER', f'Initial: {test_results[\"ber\"]:.3f}')),\n                 dbc.Col(create_card(11, '5', 'K-S', 12))\n                 ]),\n\n            dbc.Row([html.Div(dcc.Dropdown(X_test.columns, 'age', id='dropdown'),\n                              style={'width': '50%'}\n                              ),\n                     html.Div(dcc.Dropdown(X_test.columns, 'age', id='dropdown_2'),\n                              style={'width': '50%'}\n                              )\n                     ]\n                    ),\n            dbc.Row([\n                dbc.Col(\n                    dcc.Graph(id='graph-with-slider', figure=psi_variable_plot(monitoring)\n                              )\n                ),\n                dbc.Col(\n                    dcc.Graph(id='graph-distributions',\n                              figure=plot_ks(y_train.values.ravel(), scorecard.predict_proba(X_train)[:, 0])\n                              )\n                )\n            ])\n        ]),\n\n        dcc.Tab(label='Assess stability', children=[\n            dbc.Row(html.Br())\n        ])\n    ]),\n\n\n    html.Br(),\n    html.Br(),\n    html.Div(html.Button('Generate report', id='generate-report-button', n_clicks=0))\n])\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\n# @app.callback(\n#     Output('graph-distributions', 'figure'),\n#     Input('dropdown', 'value')\n# )\n# def update_hist_dist(value):\n#     fig = plot_hist_dist(train[value], test[value])\n#     return fig\n\n\n# fig_roc_auc = plot_auc_roc(y_train, scorecard.predict_proba(X_train)[:, 0])\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary.py","metadata":{},"execution_count":null,"outputs":[]}]}